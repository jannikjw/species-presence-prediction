{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-1ww88x2s because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, GlobalAveragePooling2D, Conv2D, Dense, AveragePooling2D, \\\n",
    "BatchNormalization, Normalization, Dropout, Flatten, Lambda, Input, Activation, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import schedules, SGD\n",
    "from tensorflow.keras.callbacks import Callback, LambdaCallback\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "#import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import scale\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import copy\n",
    "import threading\n",
    "import opendatasets as od\n",
    "import tempfile\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "%pylab inline --no-import-all\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from GLC.data_loading.common import load_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only uncomment if you don't have the dataset stored on disk yet\n",
    "# -> have your kaggle user credentials ready\n",
    "#data = od.download(\"https://www.kaggle.com/competitions/geolifeclef-2022-lifeclef-2022-fgvc9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path to competition dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this path to adapt to where you downloaded the data\n",
    "DATA_PATH = Path(\"./geolifeclef-2022-lifeclef-2022-fgvc9/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following two commands to verify that the data path is set correctly. They should print folder and file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmetadata\u001b[0m/      \u001b[01;34mpatches-fr\u001b[0m/  \u001b[01;34mpatches_sample\u001b[0m/  \u001b[01;34mrasters\u001b[0m/\r\n",
      "\u001b[01;34mobservations\u001b[0m/  \u001b[01;34mpatches-us\u001b[0m/  \u001b[01;34mpre-extracted\u001b[0m/   sample_submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls -L $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations_fr_test.csv   observations_us_test.csv\r\n",
      "observations_fr_train.csv  observations_us_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls $DATA_PATH/observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the observation ids of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for training: 1627475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>species_id</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observation_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10561949</th>\n",
       "      <td>45.705116</td>\n",
       "      <td>1.424622</td>\n",
       "      <td>241</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10131188</th>\n",
       "      <td>45.146973</td>\n",
       "      <td>6.416794</td>\n",
       "      <td>101</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799362</th>\n",
       "      <td>46.783695</td>\n",
       "      <td>-2.072855</td>\n",
       "      <td>700</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10392536</th>\n",
       "      <td>48.604866</td>\n",
       "      <td>-2.825003</td>\n",
       "      <td>1456</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10335049</th>\n",
       "      <td>48.815567</td>\n",
       "      <td>-0.161431</td>\n",
       "      <td>157</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 latitude  longitude  species_id subset\n",
       "observation_id                                         \n",
       "10561949        45.705116   1.424622         241  train\n",
       "10131188        45.146973   6.416794         101  train\n",
       "10799362        46.783695  -2.072855         700  train\n",
       "10392536        48.604866  -2.825003        1456  train\n",
       "10335049        48.815567  -0.161431         157  train"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training Dataset ###\n",
    "# let's load the data from file\n",
    "df_obs_fr = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "df_obs = pd.concat((df_obs_fr, df_obs_us))\n",
    "\n",
    "print(\"Number of observations for training: {}\".format(len(df_obs)))\n",
    "\n",
    "# let's have a look at the data\n",
    "df_obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the observation ids of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for testing: 36421\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observation_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10782781</th>\n",
       "      <td>43.601788</td>\n",
       "      <td>6.940195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10364138</th>\n",
       "      <td>46.241711</td>\n",
       "      <td>0.683586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10692017</th>\n",
       "      <td>45.181095</td>\n",
       "      <td>1.533459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10222322</th>\n",
       "      <td>46.938450</td>\n",
       "      <td>5.298678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10241950</th>\n",
       "      <td>45.017433</td>\n",
       "      <td>0.960736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 latitude  longitude\n",
       "observation_id                      \n",
       "10782781        43.601788   6.940195\n",
       "10364138        46.241711   0.683586\n",
       "10692017        45.181095   1.533459\n",
       "10222322        46.938450   5.298678\n",
       "10241950        45.017433   0.960736"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test Dataset ###\n",
    "df_obs_fr_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "df_obs_test = pd.concat((df_obs_fr_test, df_obs_us_test))\n",
    "\n",
    "print(\"Number of observations for testing: {}\".format(len(df_obs_test)))\n",
    "\n",
    "df_obs_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load suggested landcover alignment (only relevant if you're using landcover data later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   landcover_code  suggested_landcover_code suggested_landcover_label\n",
      "0               0                         0              Missing Data\n",
      "1               1                        11          Cultivated Crops\n",
      "2               2                        11          Cultivated Crops\n",
      "3               3                         6       Broad-leaved Forest\n",
      "4               4                         7         Coniferous Forest\n"
     ]
    }
   ],
   "source": [
    "df_suggested_landcover_alignment = pd.read_csv(DATA_PATH / \"metadata\" / \"landcover_suggested_alignment.csv\", sep=\";\")\n",
    "print(df_suggested_landcover_alignment.head())\n",
    "landcover_mapping = df_suggested_landcover_alignment[\"suggested_landcover_code\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Split Labels\n",
    "Retrieve the train/val split provided, and load the labels of the train and val set elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1587395 (97.5% of train observations)\n",
      "Validation set size: 40080 (2.5% of train observations)\n"
     ]
    }
   ],
   "source": [
    "obs_id_train = df_obs.index[df_obs[\"subset\"] == \"train\"].values\n",
    "obs_id_val = df_obs.index[df_obs[\"subset\"] == \"val\"].values\n",
    "\n",
    "y_train = df_obs.loc[obs_id_train][\"species_id\"].values\n",
    "y_val = df_obs.loc[obs_id_val][\"species_id\"].values\n",
    "\n",
    "n_val = len(obs_id_val)\n",
    "print(\"Training set size: {} ({:.1%} of train observations)\".format(len(y_train), len(y_train) / len(df_obs)))\n",
    "print(\"Validation set size: {} ({:.1%} of train observations)\".format(n_val, n_val / len(df_obs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's have a look at the training dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Dataset ===\n",
      "There are 17031 unique labels.\n",
      "We have 93.21 observations per label on average.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Training Dataset ===\")\n",
    "print(\"There are {} unique labels.\".format(len(np.unique(y_train))))\n",
    "print(\"We have {:.2f} observations per label on average.\".format(len(obs_id_train)/len(np.unique(y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# label_count = defaultdict(lambda: 0)\n",
    "\n",
    "# print(\"counting observation ids per label ...\")\n",
    "# for ob in df_obs.iterrows():\n",
    "#     label_count[ob[1]['species_id']] += 1\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buckets = [0, 10, 100, 500, 1000, 2000, 3000, 10000, 100000]\n",
    "# buckets_counts = {0: 0, 10: 0, 100: 0, 500: 0, 1000: 0, 2000: 0, 3000: 0, 10000: 0, 100000: 0}\n",
    "# buckets_labels = {0: list(), 10: list(), 100: list(), 500: list(), 1000: list(), \n",
    "#                   2000: list(), 3000: list(), 10000: list(), 100000: list()}\n",
    "\n",
    "# for label in label_count:\n",
    "#     c = label_count[label]\n",
    "    \n",
    "#     i = 0\n",
    "#     while c > buckets[i]:\n",
    "#         i += 1\n",
    "#     i -= 1\n",
    "        \n",
    "#     #print(\"c: \", c)\n",
    "#     #print(\"buckets[i]: \", buckets[i])\n",
    "    \n",
    "#     buckets_counts[buckets[i]] += 1\n",
    "#     buckets_labels[buckets[i]].append(label)\n",
    "    \n",
    "# print(\"=== Analysis of observation ids per label in training set ===\")\n",
    "# print(\"{:<15}: # of labels that have that range amount of obs ids\".format(\"# of obs ids\"))\n",
    "# print()\n",
    "# for i in range(0, len(buckets)-1):\n",
    "#     print(\"{:<15}: {amount:>4} labels\".format(\"{lower} to {upper}\".format(lower=buckets[i], upper=buckets[i+1]), \n",
    "#                                               amount=buckets_counts[buckets[i]]))\n",
    "\n",
    "# #print(buckets_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only retrieve the data belonging to a subset of all possible labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### Keep only the labels with lots of observations ###\n",
    "# min_obs_count = 2000\n",
    "# max_obs_count = 3000\n",
    "\n",
    "# # obtain all relevant labels\n",
    "# y_train = list()\n",
    "# for b in buckets:\n",
    "#     if b >= min_obs_count and b <= max_obs_count:\n",
    "#         for l in buckets_labels[b]:\n",
    "#             y_train.append(l)\n",
    "# print(\"obtained all relevant labels\")\n",
    "            \n",
    "# # obtain all corresponding obs ids\n",
    "# obs_list = list()\n",
    "\n",
    "# # iterate over a subset of the labels\n",
    "# counter = 0\n",
    "# for y in y_train:\n",
    "#     # for each label, retrieve all corresponding observation ids\n",
    "#     obs = df_obs.index[(df_obs[\"subset\"] == \"train\") & (df_obs[\"species_id\"] == y)].values\n",
    "#     #print(len(obs))\n",
    "#     obs_list.append(obs)\n",
    "    \n",
    "# # we now have a numpy array of all observation ids corresponding to this subset of labels\n",
    "# obs_id_train_new = np.concatenate(obs_list)\n",
    "# print(\"obtained all observations\")\n",
    "\n",
    "# # obtain the labels in the right order \n",
    "# y_train = df_obs.loc[obs_id_train_new][\"species_id\"].values\n",
    "\n",
    "# print(len(obs_id_train_new))\n",
    "# print(len(obs_id_train))\n",
    "\n",
    "# # count how many ids exist in obs_id_train_new that don't exist in obs_id_train\n",
    "# # should be 0\n",
    "# print(\"should be 0: \", len(obs_id_train_new) - len(obs_id_train_new[np.in1d(obs_id_train_new, obs_id_train)]))\n",
    "\n",
    "# print(len(np.unique(y_train)))\n",
    "# print(np.unique(y_train))\n",
    "\n",
    "# obs_id_train = obs_id_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Kennedy's Train, Val and Test Split ###\n",
    "# CHOOSE SUBSET FOR 30 Labels\n",
    "label_amount = 30\n",
    "\n",
    "import random\n",
    "subset_size = 0\n",
    "obs_list = list()\n",
    "obs_test_list = list()\n",
    "import numpy as np\n",
    "# iterate over a subset of the labels\n",
    "m = 0\n",
    "for y in (np.unique(y_train)[:]):\n",
    "    #print(\"in\")\n",
    "    # for each label, retrieve all corresponding observation ids\n",
    "    obs = df_obs.index[(df_obs[\"species_id\"] == y)]\n",
    "   \n",
    "    #print(len(obs))\n",
    "    #print(counter)\n",
    "    #print(obs)\n",
    "    #print(len(obs))\n",
    "    if (len(obs) >= 2000 and len(obs) <= 3000):\n",
    "      t = set(df_obs.index[(df_obs[\"species_id\"] == y) & (df_obs[\"subset\"] == \"train\")].values)\n",
    "      m += len(t)\n",
    "      ten_perc = int(len(t)/10)\n",
    "      random.seed(3)\n",
    "      test = random.sample(t, ten_perc)\n",
    "      train = t-set(test)\n",
    "      #print(train)\n",
    "      #print(\"here\")\n",
    "      obs_test_list.append(list(test))\n",
    "      obs_list.append(list(train))\n",
    "      subset_size += 1\n",
    "    if (subset_size >= label_amount):\n",
    "      #print(\"break\")\n",
    "      break\n",
    "print(m)   \n",
    "# we now have a numpy array of all observation ids corresponding to this subset of labels\n",
    "obs_id_train = np.concatenate(obs_list)\n",
    "obs_id_test = np.concatenate(obs_test_list)\n",
    "gps_train = np.concatenate((df_obs.loc[obs_id_train][\"latitude\"].values, df_obs.loc[obs_id_train][\"longitude\"].values))\n",
    "# obtain the labels in the right order \n",
    "y_train = df_obs.loc[obs_id_train][\"species_id\"].values\n",
    "y_test = df_obs.loc[obs_id_test][\"species_id\"].values\n",
    "print(y_train.size)\n",
    "\n",
    "print()\n",
    "print(y_test.size)\n",
    "print(y_train[2])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_size = 8\n",
    "obs_list_1 = list()\n",
    "print(y_val.size)\n",
    "\n",
    "# iterate over a subset of the labels\n",
    "counter = 0\n",
    "print(y_val[0])\n",
    "for y in (np.unique(y_val)[:]):\n",
    "    # for each label, retrieve all corresponding observation ids\n",
    "    if (y in y_train):\n",
    "      v = df_obs.index[(df_obs[\"species_id\"] == y) & (df_obs[\"subset\"] == \"val\")].values\n",
    "      obs_list_1.append(v)\n",
    "    \n",
    "# we now have a numpy array of all observation ids corresponding to this subset of labels\n",
    "obs_id_val = np.concatenate(obs_list_1)\n",
    "\n",
    "# obtain the labels in the right order \n",
    "y_val = df_obs.loc[obs_id_val][\"species_id\"].values\n",
    "gps_val = np.concatenate((df_obs.loc[obs_id_val][\"latitude\"].values, df_obs.loc[obs_id_val][\"longitude\"].values))\n",
    "\n",
    "print(y_val.size)\n",
    "print(obs_id_val == obs_id_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict_count = defaultdict(lambda: 0)\n",
    "for y in y_train:\n",
    "    train_dict_count[y] += 1\n",
    "    \n",
    "print(\"training: \")\n",
    "for key, value in train_dict_count.items():\n",
    "    print(\"label {:>4}: {:.2f}%\".format(key, value/len(y_train)))\n",
    "\n",
    "print()\n",
    "\n",
    "val_dict_count = defaultdict(lambda: 0)\n",
    "for y in y_val:\n",
    "    val_dict_count[y] += 1\n",
    "    \n",
    "print(\"validation: \")\n",
    "for key, value in val_dict_count.items():\n",
    "    print(\"label {:>4}: {:.2f}%\".format(key, value/len(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remap the labels s.t. they go from 0 to n-1\n",
    "(NAN fix is here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map s.t. the labels will go from 0 to n-1\n",
    "map_labels = dict()\n",
    "i = 0\n",
    "for l in np.unique(y_train):\n",
    "    map_labels[l] = i\n",
    "    i+=1\n",
    "print(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the map to the training labels\n",
    "y_train_normalized = np.zeros(np.shape(y_train), dtype='int64')\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    y_train_normalized[i] = map_labels[y_train[i]]\n",
    "    \n",
    "# shuffle together\n",
    "obs_id_train, y_train = shuffle(obs_id_train, y_train_normalized)\n",
    "\n",
    "no_output_neurons = len(np.unique(y_train))\n",
    "print(\"# output neurons: \", no_output_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the map to the validation labels\n",
    "y_val_normalized = np.zeros(np.shape(y_val), dtype='int64')\n",
    "\n",
    "for i in range(len(y_val)):\n",
    "    y_val_normalized[i] = map_labels[y_val[i]]\n",
    "    \n",
    "# shuffle together\n",
    "obs_id_val, y_val = shuffle(obs_id_val, y_val_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the map to the test labels\n",
    "y_test_normalized = np.zeros(np.shape(y_test), dtype='int64')\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    y_test_normalized[i] = map_labels[y_test[i]]\n",
    "    \n",
    "# shuffle together\n",
    "obs_id_test, y_test = shuffle(obs_id_test, y_test_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write input pipeline to load batches as we train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Generator\n",
    "Since dataset is too large to load it all into memory once, we need to load it from disk in batches as we train. Such a generator can later be passed into model.fit() instead of a train and/or validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches_Generator(tf.keras.utils.Sequence) :\n",
    "  \n",
    "    def __init__(self, obs_ids, labels, batch_size) :\n",
    "        self.obs_ids = obs_ids\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # to make the generator thread safe \n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.obs_ids) / float(self.batch_size))).astype(int)\n",
    "  \n",
    "    # returns one batch\n",
    "    def __getitem__(self, idx) :\n",
    "        X_batch = list()\n",
    "        y_batch = list()\n",
    "\n",
    "        for i in range(idx * self.batch_size, (idx+1) * self.batch_size):\n",
    "            if i >= len(self.obs_ids): break\n",
    "            \n",
    "            patch = load_patch(self.obs_ids[i], DATA_PATH, data='rgb')\n",
    "            X_batch.append(patch[0])\n",
    "            y_batch.append(self.labels[i])\n",
    "\n",
    "        with self.lock:\n",
    "            return np.asarray(X_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Simple Neural Network\n",
    "Let's create a first neural network as a baseline to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for distributed training (that is, using multiple GPUs for data parallelization)\n",
    "# # https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a simple convolutional neural net\n",
    "def complex_model(input_shape, learning_rate=0.1, output_neurons=46):\n",
    "    \n",
    "    # for distributed training\n",
    "    #with mirrored_strategy.scope():\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    dropout_prob = 0.1\n",
    "    \n",
    "    random.seed(42)\n",
    "\n",
    "    he = tf.keras.initializers.HeNormal(seed=42)\n",
    "    \n",
    "    # 1. Preprocessing\n",
    "    # rescale inputs to distribution with mean = 0 and variance = 1\n",
    "    #model.add(tf.keras.layers.Normalization())\n",
    "    model.add(tf.keras.layers.Rescaling(1./255))\n",
    "    #model.add(tf.keras.layers.RandomFlip(\"horizontal\"))\n",
    "    #model.add(tf.keras.layers.RandomRotation(factor=0.02))\n",
    "    model.add(tf.keras.layers.RandomContrast(factor=0.1))\n",
    "    model.add(tf.keras.layers.RandomCrop(input_shape[0], input_shape[1]))\n",
    "    #model.add(tf.keras.layers.RandomZoom(-0.1, 0.1))\n",
    "\n",
    "    # 2. Convolutional Layers\n",
    "    # 64 units\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', padding='valid', input_shape=input_shape,\n",
    "                     kernel_initializer=he))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', padding='valid',\n",
    "                     kernel_initializer=he))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    \n",
    "    # 128 units\n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='valid',\n",
    "                     kernel_initializer=he))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='valid',\n",
    "                     kernel_initializer=he))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='valid',\n",
    "                     kernel_initializer=he))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    \n",
    "    # 256 units\n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='valid',\n",
    "                     kernel_initializer=he))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='valid',\n",
    "                     kernel_initializer=he))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(dropout_prob))\n",
    "\n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='valid',\n",
    "                     kernel_initializer=he))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(dropout_prob))\n",
    "\n",
    "    \n",
    "    # from convolutional layers to dense layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    \n",
    "    # 3. Dense Layers\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer=he))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    \n",
    "    model.add(Dense(128, activation='relu', kernel_initializer=he))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    \n",
    "    model.add(Dense(256, activation='relu', kernel_initializer=he))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "\n",
    "    # 4. Output Layer\n",
    "    model.add(Dense(output_neurons, activation='softmax'))\n",
    "    \n",
    "    # compire the model\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "                           tf.keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\")])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings used when parallelizing the I/O Pipeline in model.fit().\n",
    "\n",
    "I used the setting values in the cell below for a high-cpu vm on GCP with the following specs:\n",
    "- machine type: n1-highcpu-96 (96 CPU cores)\n",
    "- vCPUs to core ratio: 2 vCPUs per core (making a theoretical max value for num_threads of 96 * 2 = 192)\n",
    "- 4 x NVIDIA Tesla T4 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to tune the learning rate accordingly.\n",
    "BATCHSIZE = 64\n",
    "\n",
    "# The maximun value for num_threads is dependent on amount of CPU cores:\n",
    "# amount of CPU cores * vCPUs to core ratio = theoretical max of NUM_THREADS\n",
    "NUM_THREADS = 11\n",
    "\n",
    "# The more batches we prefetch, the less idle the GPUs will be. \n",
    "# To check GPU usage:\n",
    "# 1. Run nvidia-smi -l 1 from the terminal to monitor the GPU usage during training. \n",
    "# 2. Try to get close to 100% for all GPUs by adjusting the value below (and the two above). Due to the overhead\n",
    "#    from tf.distribute.MirroredStrategy(), you won't be able to consistently get 100% for all GPUs. But try to \n",
    "#    get close.\n",
    "# 3. Be aware that RAM limits the amount of batches you can prefetch.\n",
    "PRE_FETCH_NUM_BATCHES = int(NUM_THREADS * 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators\n",
    "Create generators that will read training / validation data from disk during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = Patches_Generator(obs_id_train, y_train, BATCHSIZE)\n",
    "#train_gps_generator = GPS_Generator(obs_id_train, gps_train, y_train, BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = Patches_Generator(obs_id_val, y_val, BATCHSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclic learning rate\n",
    "Tune the cyclic learning rate prior to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code source: https://pyimagesearch.com/2019/08/05/keras-learning-rate-finder/\n",
    "\n",
    "class LearningRateFinder:\n",
    "    def __init__(self, model, stopFactor=4, beta=0.98):\n",
    "        # store the model, stop factor, and beta value (for computing\n",
    "        # a smoothed, average loss)\n",
    "        self.model = model\n",
    "        self.stopFactor = stopFactor\n",
    "        self.beta = beta\n",
    "        \n",
    "        # initialize our list of learning rates and losses,\n",
    "        # respectively\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        \n",
    "        # initialize our learning rate multiplier, average loss, best\n",
    "        # loss found thus far, current batch number, and weights file\n",
    "        self.lrMult = 1\n",
    "        self.avgLoss = 0\n",
    "        self.bestLoss = 1e9\n",
    "        self.batchNum = 0\n",
    "        self.weightsFile = None\n",
    "        \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # grab the current learning rate and add log it to the list of\n",
    "        # learning rates that we've tried\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "        \n",
    "        # grab the loss at the end of this batch, increment the total\n",
    "        # number of batches processed, compute the average average\n",
    "        # loss, smooth it, and update the losses list with the\n",
    "        # smoothed value\n",
    "        l = logs[\"loss\"]\n",
    "        self.batchNum += 1\n",
    "        self.avgLoss = (self.beta * self.avgLoss) + ((1 - self.beta) * l)\n",
    "        smooth = self.avgLoss / (1 - (self.beta ** self.batchNum))\n",
    "        self.losses.append(smooth)\n",
    "        \n",
    "        # compute the maximum loss stopping factor value\n",
    "        stopLoss = self.stopFactor * self.bestLoss\n",
    "        \n",
    "        # check to see whether the loss has grown too large\n",
    "        if self.batchNum > 1 and smooth > stopLoss:\n",
    "            # stop returning and return from the method\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "        \n",
    "        # check to see if the best loss should be updated\n",
    "        if self.batchNum == 1 or smooth < self.bestLoss:\n",
    "            self.bestLoss = smooth\n",
    "            \n",
    "        # increase the learning rate\n",
    "        lr *= self.lrMult\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "    def find(self, trainData, startLR, endLR, epochs=None,\n",
    "        stepsPerEpoch=None, batchSize=64, sampleSize=2048):\n",
    "            \n",
    "        # compute the total number of batch updates that will take\n",
    "        # place while we are attempting to find a good starting\n",
    "        # learning rate\n",
    "        numBatchUpdates = epochs * stepsPerEpoch\n",
    "        \n",
    "        # derive the learning rate multiplier based on the ending\n",
    "        # learning rate, starting learning rate, and total number of\n",
    "        # batch updates\n",
    "        self.lrMult = (endLR / startLR) ** (1.0 / numBatchUpdates)\n",
    "        \n",
    "        # grab the *original* learning rate (so we can reset it\n",
    "        # later), and then set the *starting* learning rate\n",
    "        origLR = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, startLR)\n",
    "\n",
    "        # construct a callback that will be called at the end of each\n",
    "        # batch, enabling us to increase our learning rate as training\n",
    "        # progresses\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch, logs:\n",
    "            self.on_batch_end(batch, logs))\n",
    "        \n",
    "        # train our model using Keras' fit method\n",
    "        \"\"\"self.model.fit(\n",
    "            trainData,\n",
    "            batch_size=batchSize,\n",
    "            epochs=epochs,\n",
    "            callbacks=[callback])\"\"\"\n",
    "        \n",
    "        history = self.model.fit(trainData, \n",
    "                            batch_size=batchSize,\n",
    "                            epochs=epochs, \n",
    "                            steps_per_epoch=stepsPerEpoch,\n",
    "                            callbacks=[callback], \n",
    "                            # for parallelization of reading from disk (I/O) pipeline\n",
    "                            max_queue_size=PRE_FETCH_NUM_BATCHES, \n",
    "                            workers=NUM_THREADS, \n",
    "                            use_multiprocessing=True)\n",
    "        \n",
    "        # create a temporary file path for the model weights and\n",
    "        # then save the weights (so we can reset the weights when we\n",
    "        # are done)\n",
    "        self.weightsFile = tempfile.mkstemp()[1]\n",
    "        self.model.save_weights(self.weightsFile)\n",
    "            \n",
    "        # restore the original model weights and learning rate\n",
    "        self.model.load_weights(self.weightsFile)\n",
    "        K.set_value(self.model.optimizer.lr, origLR)\n",
    "\n",
    "    def plot_loss(self, skipBegin=10, skipEnd=1, title=\"\", show_candidates_only=True):\n",
    "        # grab the learning rate and losses values to plot\n",
    "        lrs = self.lrs[skipBegin:-skipEnd]\n",
    "        losses = self.losses[skipBegin:-skipEnd]\n",
    "        \n",
    "        # extract the candidate losses\n",
    "        candidates = [1e-15, 1e-14, 1e-13, 1e-12, 1e-11, 1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e+0, 1e+1, 1e+2]\n",
    "\n",
    "        x_vals = list()\n",
    "        y_vals = list()\n",
    "        nextCandidate = 0\n",
    "        \n",
    "        for i in range(len(lrs)):\n",
    "            if math.isclose(lrs[i], candidates[nextCandidate], rel_tol=0.09, abs_tol=0):\n",
    "                x_vals.append(candidates[nextCandidate])\n",
    "                y_vals.append(losses[i])\n",
    "                nextCandidate += 1\n",
    "            \n",
    "            if nextCandidate >= len(candidates): \n",
    "                break\n",
    "                \n",
    "        x_vals.pop()\n",
    "        y_vals.pop()\n",
    "        x_vals.append(candidates[nextCandidate-1])\n",
    "        y_vals.append(losses[-1])\n",
    "        \n",
    "        # plot the learning rate vs. loss\n",
    "        if show_candidates_only:\n",
    "            plt.title(\"10 Candidate Learning Rates\")\n",
    "            plt.plot(x_vals, y_vals)\n",
    "        else:\n",
    "            plt.title(\"All Analyzed Learning Rates\")\n",
    "            plt.plot(lrs, losses)\n",
    "        \n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Learning Rate (Log Scale)\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid()\n",
    "        plt.xticks(candidates)\n",
    "        \n",
    "        # if the title is not empty, add it to the plot\n",
    "        if title != \"\":\n",
    "            plt.title(title)\n",
    "            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] finding learning rate...\n",
      "Epoch 1/6\n",
      "989/989 [==============================] - 158s 155ms/step - loss: 6.2518 - accuracy: 0.0355\n",
      "Epoch 2/6\n",
      "989/989 [==============================] - 158s 156ms/step - loss: 6.2761 - accuracy: 0.0347\n",
      "Epoch 3/6\n",
      "989/989 [==============================] - 163s 160ms/step - loss: 6.0520 - accuracy: 0.0341\n",
      "Epoch 4/6\n",
      "989/989 [==============================] - 160s 156ms/step - loss: 3.6846 - accuracy: 0.0360\n",
      "Epoch 5/6\n",
      "989/989 [==============================] - 107s 103ms/step - loss: 18.8239 - accuracy: 0.0381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_ForkPoolWorker-280:\n",
      "Process Keras_worker_ForkPoolWorker-278:\n",
      "Process Keras_worker_ForkPoolWorker-287:\n",
      "Process Keras_worker_ForkPoolWorker-286:\n",
      "Process Keras_worker_ForkPoolWorker-283:\n",
      "Process Keras_worker_ForkPoolWorker-282:\n",
      "Process Keras_worker_ForkPoolWorker-284:\n",
      "Process Keras_worker_ForkPoolWorker-277:\n",
      "Process Keras_worker_ForkPoolWorker-285:\n",
      "Process Keras_worker_ForkPoolWorker-279:\n",
      "Process Keras_worker_ForkPoolWorker-281:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 356, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # uncomment if you want to analyze which upper and lower bound for the cyclical learning rate policy to use\n",
    "\n",
    "# ###### initialize model\n",
    "# model_lr = complex_model((256, 256, 3), learning_rate=1e-9, output_neurons=no_output_neurons)\n",
    "\n",
    "# # initialize the learning rate finder and then train with learning\n",
    "# # rates ranging from 1e-10 to 1e+1\n",
    "# print(\"[INFO] finding learning rate...\")\n",
    "# lrf = LearningRateFinder(model_lr)\n",
    "# lrf.find(train_generator,\n",
    "#          1e-15, 1e+2,\n",
    "#          stepsPerEpoch=np.ceil(len(y_train)/BATCHSIZE),\n",
    "#          epochs=6, \n",
    "#          batchSize=BATCHSIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss for the candidate learning rates\n",
    "lrf.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss for the various learning rates\n",
    "lrf.plot_loss(show_candidates_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From that figure identify the values of lrmin and lrmax.\n",
    "\n",
    "I have identified them as follows:\n",
    "\n",
    "    lr_min = 1e-7\n",
    "    lr_max = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cyclical learning rate policy (with exponential decay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = len(y_train)//BATCHSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cyclical learning rate policy\n",
    "lr_min = 1e-7\n",
    "lr_max = 1e-3\n",
    "\n",
    "clr = tfa.optimizers.CyclicalLearningRate(\n",
    "        initial_learning_rate=lr_min,\n",
    "        maximal_learning_rate=lr_max,\n",
    "        scale_fn=lambda x: 1/(2.**(x-1)), # exponential decay\n",
    "        step_size=2 * STEPS_PER_EPOCH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network\n",
    "#model = simple_model((256, 256, 3), learning_rate=clr, output_neurons=len(np.unique(y_train)))\n",
    "#model = simple_model_with_gps([(256, 256, 3), np.shape(gps)], learning_rate=clr, output_neurons=len(np.unique(y_train)))\n",
    "\n",
    "model = complex_model((256, 256, 3), learning_rate=clr, output_neurons=no_output_neurons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_1 (Rescaling)     (64, 256, 256, 3)         0         \n",
      "                                                                 \n",
      " random_contrast_1 (RandomCo  (64, 256, 256, 3)        0         \n",
      " ntrast)                                                         \n",
      "                                                                 \n",
      " random_crop_1 (RandomCrop)  (64, 256, 256, 3)         0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (64, 254, 254, 64)        1792      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (64, 254, 254, 64)        0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (64, 252, 252, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (64, 126, 126, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (64, 126, 126, 64)        0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (64, 124, 124, 128)       73856     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (64, 124, 124, 128)       0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (64, 122, 122, 128)       147584    \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (64, 122, 122, 128)       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (64, 120, 120, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (64, 60, 60, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (64, 60, 60, 128)         0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (64, 58, 58, 256)         295168    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (64, 29, 29, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (64, 29, 29, 256)         0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (64, 27, 27, 256)         590080    \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (64, 13, 13, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (64, 13, 13, 256)         0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (64, 11, 11, 256)         590080    \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (64, 5, 5, 256)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (64, 5, 5, 256)           0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (64, 6400)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (64, 64)                  409664    \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (64, 64)                  0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (64, 128)                 8320      \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (64, 128)                 0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (64, 256)                 33024     \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (64, 256)                 0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (64, 30)                  7710      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,341,790\n",
      "Trainable params: 2,341,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=30, \n",
    "                                              verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "989/989 [==============================] - 337s 310ms/step - loss: 3.4260 - accuracy: 0.0380 - top-5-accuracy: 0.1849 - val_loss: 3.3992 - val_accuracy: 0.0410 - val_top-5-accuracy: 0.1420\n",
      "Epoch 2/100\n",
      "989/989 [==============================] - 136s 137ms/step - loss: 3.3978 - accuracy: 0.0384 - top-5-accuracy: 0.1902 - val_loss: 3.4014 - val_accuracy: 0.0339 - val_top-5-accuracy: 0.1480\n",
      "Epoch 3/100\n",
      "989/989 [==============================] - 134s 134ms/step - loss: 3.3968 - accuracy: 0.0384 - top-5-accuracy: 0.1905 - val_loss: 3.4008 - val_accuracy: 0.0339 - val_top-5-accuracy: 0.1496\n",
      "Epoch 4/100\n",
      "989/989 [==============================] - 137s 136ms/step - loss: 3.3569 - accuracy: 0.0579 - top-5-accuracy: 0.2118 - val_loss: 3.3686 - val_accuracy: 0.0628 - val_top-5-accuracy: 0.2332\n",
      "Epoch 5/100\n",
      "989/989 [==============================] - 137s 136ms/step - loss: 3.2327 - accuracy: 0.0952 - top-5-accuracy: 0.2926 - val_loss: 3.1923 - val_accuracy: 0.0945 - val_top-5-accuracy: 0.3321\n",
      "Epoch 6/100\n",
      "989/989 [==============================] - 137s 136ms/step - loss: 3.0773 - accuracy: 0.1198 - top-5-accuracy: 0.3682 - val_loss: 3.1170 - val_accuracy: 0.0918 - val_top-5-accuracy: 0.3779\n",
      "Epoch 7/100\n",
      "989/989 [==============================] - 137s 135ms/step - loss: 2.9629 - accuracy: 0.1438 - top-5-accuracy: 0.4175 - val_loss: 2.9398 - val_accuracy: 0.1453 - val_top-5-accuracy: 0.4413\n",
      "Epoch 8/100\n",
      "989/989 [==============================] - 140s 140ms/step - loss: 2.8081 - accuracy: 0.1814 - top-5-accuracy: 0.4855 - val_loss: 2.8658 - val_accuracy: 0.1644 - val_top-5-accuracy: 0.4823\n",
      "Epoch 9/100\n",
      "989/989 [==============================] - 137s 136ms/step - loss: 2.7405 - accuracy: 0.1984 - top-5-accuracy: 0.5125 - val_loss: 2.9110 - val_accuracy: 0.1349 - val_top-5-accuracy: 0.4637\n",
      "Epoch 10/100\n",
      "989/989 [==============================] - 139s 137ms/step - loss: 2.7310 - accuracy: 0.2000 - top-5-accuracy: 0.5156 - val_loss: 2.8695 - val_accuracy: 0.1546 - val_top-5-accuracy: 0.4631\n",
      "Epoch 11/100\n",
      "989/989 [==============================] - 139s 138ms/step - loss: 2.6571 - accuracy: 0.2206 - top-5-accuracy: 0.5467 - val_loss: 2.8368 - val_accuracy: 0.1704 - val_top-5-accuracy: 0.5025\n",
      "Epoch 12/100\n",
      "989/989 [==============================] - 136s 136ms/step - loss: 2.5013 - accuracy: 0.2600 - top-5-accuracy: 0.5998 - val_loss: 2.8474 - val_accuracy: 0.1573 - val_top-5-accuracy: 0.5079\n",
      "Epoch 13/100\n",
      "989/989 [==============================] - 140s 138ms/step - loss: 2.4083 - accuracy: 0.2852 - top-5-accuracy: 0.6314 - val_loss: 2.8726 - val_accuracy: 0.1535 - val_top-5-accuracy: 0.5014\n",
      "Epoch 14/100\n",
      "989/989 [==============================] - 137s 136ms/step - loss: 2.3952 - accuracy: 0.2876 - top-5-accuracy: 0.6347 - val_loss: 2.8945 - val_accuracy: 0.1573 - val_top-5-accuracy: 0.5014\n",
      "Epoch 15/100\n",
      "989/989 [==============================] - 139s 137ms/step - loss: 2.2966 - accuracy: 0.3134 - top-5-accuracy: 0.6674 - val_loss: 3.0041 - val_accuracy: 0.1513 - val_top-5-accuracy: 0.4773\n",
      "Epoch 16/100\n",
      "989/989 [==============================] - 139s 138ms/step - loss: 2.0950 - accuracy: 0.3662 - top-5-accuracy: 0.7240 - val_loss: 3.1521 - val_accuracy: 0.1382 - val_top-5-accuracy: 0.4708\n",
      "Epoch 17/100\n",
      "989/989 [==============================] - 142s 140ms/step - loss: 2.0003 - accuracy: 0.3948 - top-5-accuracy: 0.7481 - val_loss: 3.2499 - val_accuracy: 0.1458 - val_top-5-accuracy: 0.4599\n",
      "Epoch 18/100\n",
      "989/989 [==============================] - 137s 136ms/step - loss: 1.9933 - accuracy: 0.3941 - top-5-accuracy: 0.7497 - val_loss: 3.3545 - val_accuracy: 0.1458 - val_top-5-accuracy: 0.4620\n",
      "Epoch 19/100\n",
      "989/989 [==============================] - 138s 137ms/step - loss: 1.8996 - accuracy: 0.4216 - top-5-accuracy: 0.7728 - val_loss: 3.4911 - val_accuracy: 0.1404 - val_top-5-accuracy: 0.4539\n",
      "Epoch 20/100\n",
      "989/989 [==============================] - 139s 138ms/step - loss: 1.7463 - accuracy: 0.4656 - top-5-accuracy: 0.8061 - val_loss: 3.6593 - val_accuracy: 0.1382 - val_top-5-accuracy: 0.4522\n",
      "Epoch 21/100\n",
      "989/989 [==============================] - 140s 139ms/step - loss: 1.6827 - accuracy: 0.4869 - top-5-accuracy: 0.8200 - val_loss: 3.7483 - val_accuracy: 0.1360 - val_top-5-accuracy: 0.4468\n",
      "Epoch 22/100\n",
      "989/989 [==============================] - 140s 137ms/step - loss: 1.6852 - accuracy: 0.4849 - top-5-accuracy: 0.8188 - val_loss: 3.8594 - val_accuracy: 0.1371 - val_top-5-accuracy: 0.4276\n",
      "Epoch 23/100\n",
      "989/989 [==============================] - 137s 135ms/step - loss: 1.6254 - accuracy: 0.5006 - top-5-accuracy: 0.8306 - val_loss: 3.9973 - val_accuracy: 0.1344 - val_top-5-accuracy: 0.4364\n",
      "Epoch 24/100\n",
      "989/989 [==============================] - 137s 136ms/step - loss: 1.5321 - accuracy: 0.5312 - top-5-accuracy: 0.8470 - val_loss: 4.1290 - val_accuracy: 0.1322 - val_top-5-accuracy: 0.4358\n",
      "Epoch 25/100\n",
      "989/989 [==============================] - 140s 138ms/step - loss: 1.4991 - accuracy: 0.5417 - top-5-accuracy: 0.8535 - val_loss: 4.2249 - val_accuracy: 0.1354 - val_top-5-accuracy: 0.4326\n",
      "Epoch 26/100\n",
      "989/989 [==============================] - 137s 136ms/step - loss: 1.4963 - accuracy: 0.5395 - top-5-accuracy: 0.8537 - val_loss: 4.3231 - val_accuracy: 0.1322 - val_top-5-accuracy: 0.4326\n",
      "Epoch 27/100\n",
      "989/989 [==============================] - 141s 140ms/step - loss: 1.4628 - accuracy: 0.5496 - top-5-accuracy: 0.8591 - val_loss: 4.3801 - val_accuracy: 0.1305 - val_top-5-accuracy: 0.4336\n",
      "Epoch 28/100\n",
      "989/989 [==============================] - 137s 136ms/step - loss: 1.4117 - accuracy: 0.5673 - top-5-accuracy: 0.8698 - val_loss: 4.4733 - val_accuracy: 0.1289 - val_top-5-accuracy: 0.4260\n",
      "Epoch 29/100\n",
      "989/989 [==============================] - 137s 136ms/step - loss: 1.3908 - accuracy: 0.5762 - top-5-accuracy: 0.8715 - val_loss: 4.5327 - val_accuracy: 0.1305 - val_top-5-accuracy: 0.4211\n",
      "Epoch 30/100\n",
      "989/989 [==============================] - 136s 136ms/step - loss: 1.3867 - accuracy: 0.5753 - top-5-accuracy: 0.8722 - val_loss: 4.6175 - val_accuracy: 0.1267 - val_top-5-accuracy: 0.4227\n",
      "Epoch 31/100\n",
      "989/989 [==============================] - 140s 139ms/step - loss: 1.3725 - accuracy: 0.5803 - top-5-accuracy: 0.8745 - val_loss: 4.6651 - val_accuracy: 0.1256 - val_top-5-accuracy: 0.4211\n",
      "Epoch 32/100\n",
      "989/989 [==============================] - 140s 138ms/step - loss: 1.3424 - accuracy: 0.5902 - top-5-accuracy: 0.8801 - val_loss: 4.7213 - val_accuracy: 0.1294 - val_top-5-accuracy: 0.4271\n",
      "Epoch 33/100\n",
      "989/989 [==============================] - 137s 136ms/step - loss: 1.3274 - accuracy: 0.5944 - top-5-accuracy: 0.8820 - val_loss: 4.7683 - val_accuracy: 0.1256 - val_top-5-accuracy: 0.4211\n",
      "Epoch 34/100\n",
      "989/989 [==============================] - 137s 136ms/step - loss: 1.3304 - accuracy: 0.5941 - top-5-accuracy: 0.8825 - val_loss: 4.7912 - val_accuracy: 0.1294 - val_top-5-accuracy: 0.4233\n",
      "Epoch 35/100\n",
      "989/989 [==============================] - 141s 140ms/step - loss: 1.3136 - accuracy: 0.5988 - top-5-accuracy: 0.8846 - val_loss: 4.8405 - val_accuracy: 0.1262 - val_top-5-accuracy: 0.4222\n",
      "Epoch 36/100\n",
      "989/989 [==============================] - 138s 137ms/step - loss: 1.2999 - accuracy: 0.6017 - top-5-accuracy: 0.8860 - val_loss: 4.8790 - val_accuracy: 0.1262 - val_top-5-accuracy: 0.4184\n",
      "Epoch 37/100\n",
      "989/989 [==============================] - 137s 136ms/step - loss: 1.2930 - accuracy: 0.6066 - top-5-accuracy: 0.8873 - val_loss: 4.8941 - val_accuracy: 0.1240 - val_top-5-accuracy: 0.4216\n",
      "Epoch 38/100\n",
      "989/989 [==============================] - 141s 140ms/step - loss: 1.2899 - accuracy: 0.6048 - top-5-accuracy: 0.8882 - val_loss: 4.9162 - val_accuracy: 0.1278 - val_top-5-accuracy: 0.4200\n",
      "Epoch 39/100\n",
      "989/989 [==============================] - 143s 141ms/step - loss: 1.2839 - accuracy: 0.6084 - top-5-accuracy: 0.8883 - val_loss: 4.9533 - val_accuracy: 0.1251 - val_top-5-accuracy: 0.4205\n",
      "Epoch 40/100\n",
      "989/989 [==============================] - 143s 141ms/step - loss: 1.2738 - accuracy: 0.6115 - top-5-accuracy: 0.8906 - val_loss: 4.9692 - val_accuracy: 0.1245 - val_top-5-accuracy: 0.4189\n",
      "Epoch 41/100\n",
      "989/989 [==============================] - 138s 136ms/step - loss: 1.2685 - accuracy: 0.6134 - top-5-accuracy: 0.8911 - val_loss: 4.9969 - val_accuracy: 0.1267 - val_top-5-accuracy: 0.4184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_ForkPoolWorker-1476:\n",
      "Process Keras_worker_ForkPoolWorker-1480:\n",
      "Process Keras_worker_ForkPoolWorker-1481:\n",
      "Process Keras_worker_ForkPoolWorker-1477:\n",
      "Process Keras_worker_ForkPoolWorker-1483:\n",
      "Process Keras_worker_ForkPoolWorker-1482:\n",
      "Process Keras_worker_ForkPoolWorker-1478:\n",
      "Process Keras_worker_ForkPoolWorker-1479:\n",
      "Process Keras_worker_ForkPoolWorker-1484:\n",
      "Process Keras_worker_ForkPoolWorker-1475:\n",
      "Process Keras_worker_ForkPoolWorker-1485:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 356, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, epochs=100, callbacks=[early_stop], #steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    validation_data=val_generator,\n",
    "                    # for parallelization of reading from disk (I/O) pipeline\n",
    "                    max_queue_size=PRE_FETCH_NUM_BATCHES, workers=NUM_THREADS, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: complex_cnn_trial_clr_2/assets\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'complex_cnn_trial_clr_2'\n",
    "model.save(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Get the dictionary containing each metric and the loss for each epoch\n",
    "history_dict = history.history\n",
    "# Save it under the form of a json file\n",
    "json.dump(history_dict, open(model_file_name + '_history', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GLC\t\t\t\t        nathalie_2_highlytuned_cnn.ipynb\r\n",
      "'HW3 main notebook.ipynb'\t        nathalie_3_multimodal_simple.ipynb\r\n",
      " README.md\t\t\t        nathalie_4_multimodal_complex.ipynb\r\n",
      " complex_cnn_final_data\t\t        nathalie_cyclic_learning_rate.ipynb\r\n",
      " complex_cnn_final_data_history         nathalie_first_input_pipeline.ipynb\r\n",
      " download_data.py\t\t        nathalie_simple_cnn_old.ipynb\r\n",
      " first_simple_model_random_3\t        old_trials\r\n",
      " geolifeclef-2022-lifeclef-2022-fgvc9   playground.ipynb\r\n",
      " nathalie_1_simple_cnn.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'top-5-accuracy', 'val_loss', 'val_accuracy', 'val_top-5-accuracy'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load history\n",
    "import json\n",
    "history_dict = json.load(open('complex_cnn_final_data_history', 'r'))\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14199890196323395,\n",
       " 0.14800655841827393,\n",
       " 0.14964500069618225,\n",
       " 0.2332058995962143,\n",
       " 0.33205899596214294,\n",
       " 0.3779355585575104,\n",
       " 0.44128891825675964,\n",
       " 0.4822501242160797,\n",
       " 0.46368104219436646,\n",
       " 0.46313488483428955,\n",
       " 0.5024576783180237,\n",
       " 0.507919192314148,\n",
       " 0.5013653635978699,\n",
       " 0.5013653635978699,\n",
       " 0.47733479738235474,\n",
       " 0.47078099846839905,\n",
       " 0.4598580002784729,\n",
       " 0.46204259991645813,\n",
       " 0.4538503587245941,\n",
       " 0.4522119164466858,\n",
       " 0.4467504024505615,\n",
       " 0.42763516306877136,\n",
       " 0.4363735616207123,\n",
       " 0.4358274042606354,\n",
       " 0.4325505197048187,\n",
       " 0.4325505197048187,\n",
       " 0.43364280462265015,\n",
       " 0.42599672079086304,\n",
       " 0.4210813641548157,\n",
       " 0.4227198362350464,\n",
       " 0.4210813641548157,\n",
       " 0.42708903551101685,\n",
       " 0.4210813641548157,\n",
       " 0.4232659637928009,\n",
       " 0.4221736788749695,\n",
       " 0.4183506369590759,\n",
       " 0.4216275215148926,\n",
       " 0.41998907923698425,\n",
       " 0.42053523659706116,\n",
       " 0.41889676451683044,\n",
       " 0.4183506369590759]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict['val_top-5-accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = Patches_Generator(obs_id_test, y_test, BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('complex_cnn_final_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
