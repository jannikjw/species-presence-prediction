{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "\n",
        "class Environmental_Patches_Generator(tf.keras.utils.Sequence) :\n",
        "  \n",
        "    def __init__(self, obs_ids, labels, batch_size) :z\n",
        "        self.obs_ids = obs_ids\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        #self.gps = gps\n",
        "        #self.extractor = extractor\n",
        "        #print(\"INIT\")\n",
        "        # to make the generator thread safe \n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def __len__(self) :\n",
        "        return (np.ceil(len(self.obs_ids) / float(self.batch_size))).astype(int)\n",
        "  \n",
        "    # returns one batch\n",
        "    def __getitem__(self, idx) :\n",
        "        X_batch = list()\n",
        "        y_batch = list()\n",
        "        X_env_batch = list()\n",
        "\n",
        "        #print(\"ONE BATCH\")\n",
        "        for i in range(idx * self.batch_size, (idx+1) * self.batch_size):\n",
        "            if i >= len(self.obs_ids): break\n",
        "            \n",
        "            rgb, near_ir, landcover, altitude = load_patch(self.obs_ids[i], DATA_PATH, data='all')\n",
        "            ni = near_ir.reshape(256, 256, 1)\n",
        "            lc = landcover.reshape(256, 256, 1)\n",
        "            alt = altitude.reshape(256, 256, 1)\n",
        "\n",
        "            patch = np.concatenate((rgb, ni, lc, alt), axis=2)\n",
        "\n",
        "  \n",
        "            #cs = MinMaxScaler()\n",
        "            #print(\"PATCH GENERATOR\")\n",
        "            #print((df_env.loc[self.obs_ids[i]].values).shape)\n",
        "            #print(cs.fit_transform(df_env.loc[self.obs_ids[i]].values).shape)\n",
        "            #k = input()\n",
        "            X_env_batch.append(df_env.loc[self.obs_ids[i]].values)\n",
        "            #X_env_batch.append(df_env[self.obs_ids[i], :])\n",
        "            #X_env_batch.append(cs.fit_transform(df_env.loc[self.obs_ids[i]].values.reshape(-1,1)))\n",
        "            X_batch.append(patch)\n",
        "            y_batch.append(self.labels[i])\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "        with self.lock:\n",
        "        \n",
        "            \n",
        "            #return {'input_1': np.asarray(X_batch), 'input_2': np.asarray(X_env_batch)}, np.asarray(np.array(y_batch))\n",
        "            #return np.asarray(X_batch), np.array(y_batch)\n",
        "            return (np.asarray(X_batch), np.asarray(X_env_batch)), np.array(y_batch)"
      ],
      "metadata": {
        "id": "2cFjcJLQL1N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "\n",
        "class Test_Patches_Generator(tf.keras.utils.Sequence) :\n",
        "  \n",
        "    def __init__(self, obs_ids, batch_size) :\n",
        "        self.obs_ids = obs_ids\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # to make the generator thread safe \n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def __len__(self) :\n",
        "        return (np.ceil(len(self.obs_ids) / float(self.batch_size))).astype(int)\n",
        "  \n",
        "    # returns one batch\n",
        "    def __getitem__(self, idx) :\n",
        "        X_batch = list()\n",
        "        y_batch = list()\n",
        "\n",
        "        for i in range(idx * self.batch_size, (idx+1) * self.batch_size):\n",
        "            if i >= len(self.obs_ids): break\n",
        "            \n",
        "            patch = load_patch(self.obs_ids[i], DATA_PATH, data='rgb')\n",
        "            X_batch.append(patch[0])\n",
        "\n",
        "        with self.lock:\n",
        "            return np.asarray(X_batch)"
      ],
      "metadata": {
        "id": "mPrGVSHcHoLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pretrained Patches"
      ],
      "metadata": {
        "id": "ojcG8LZnrJeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "\n",
        "class Pretrained_Patches_Generator(tf.keras.utils.Sequence) :\n",
        "  \n",
        "    def __init__(self, obs_ids, labels, batch_size) :\n",
        "        self.obs_ids = obs_ids\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        #self.gps = gps\n",
        "        #self.extractor = extractor\n",
        "        #print(\"INIT\")\n",
        "        # to make the generator thread safe \n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def __len__(self) :\n",
        "        return (np.ceil(len(self.obs_ids) / float(self.batch_size))).astype(int)\n",
        "  \n",
        "    # returns one batch\n",
        "    def __getitem__(self, idx) :\n",
        "        X_batch_rgb = list()\n",
        "        X_batch_images = list()\n",
        "        y_batch = list()\n",
        "        X_env_batch = list()\n",
        "\n",
        "        #print(\"ONE BATCH\")\n",
        "        for i in range(idx * self.batch_size, (idx+1) * self.batch_size):\n",
        "            if i >= len(self.obs_ids): break\n",
        "            \n",
        "            rgb, near_ir, landcover, altitude = load_patch(self.obs_ids[i], DATA_PATH, data='all')\n",
        "            ni = near_ir.reshape(256, 256, 1)\n",
        "            lc = landcover.reshape(256, 256, 1)\n",
        "            alt = altitude.reshape(256, 256, 1)\n",
        "\n",
        "            patch1 = np.concatenate((ni, lc, alt), axis=2)\n",
        "\n",
        "  \n",
        "            #cs = MinMaxScaler()\n",
        "            #print(\"PATCH GENERATOR\")\n",
        "            #print((df_env.loc[self.obs_ids[i]].values).shape)\n",
        "            #print(cs.fit_transform(df_env.loc[self.obs_ids[i]].values).shape)\n",
        "            #k = input()\n",
        "            X_env_batch.append(df_env.loc[self.obs_ids[i]].values)\n",
        "            #X_env_batch.append(df_env[self.obs_ids[i], :])\n",
        "            #X_env_batch.append(cs.fit_transform(df_env.loc[self.obs_ids[i]].values.reshape(-1,1)))\n",
        "            X_batch_rgb.append(rgb)\n",
        "            X_batch_images.append(patch1)\n",
        "            y_batch.append(self.labels[i])\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "        with self.lock:\n",
        "        \n",
        "            \n",
        "            #print(len(X_batch_images))\n",
        "            #print(len(X_batch_rgb))\n",
        "            #print(np.asarray(X_batch_images).shape)\n",
        "            #print(\"AH\")\n",
        "            #return {'input_1': np.asarray(X_batch), 'input_2': np.asarray(X_env_batch)}, np.asarray(np.array(y_batch))\n",
        "            #return np.asarray(X_batch), np.array(y_batch)\n",
        "            return (np.asarray(X_batch_rgb), np.asarray(X_batch_images), np.asarray(X_env_batch)), np.array(y_batch)"
      ],
      "metadata": {
        "id": "kZNU6rSErIKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-Trained ResNet-50"
      ],
      "metadata": {
        "id": "iEow7NARi4cx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oVszMXT70bd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FROM https://chroniclesofai.com/transfer-learning-with-keras-resnet-50/\n",
        "#for layer in res_img.layers:\n",
        "from tensorflow.python.ops.image_ops_impl import rgb_to_grayscale\n",
        "num_classes = len(set(df_obs['species_id']))\n",
        "#pre_trained_res = Sequential()\n",
        "\n",
        "\n",
        "\n",
        "#        layer.trainable=False\n",
        "\n",
        "def transfer_learning(input_shape1, input_shape2, input_shape3, num_classes):\n",
        "    #print(\"MADE IT\")\n",
        "    #image = tf.image.resize(image, (INP_SIZE[0], INP_SIZE[1]))\n",
        "\n",
        "    res_img = tf.keras.applications.resnet50.ResNet50(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    #input_tensor=None,\n",
        "    input_shape=(224, 224, 3),\n",
        "    pooling='none',\n",
        "    classes=num_classes,\n",
        "    )\n",
        "    \n",
        "    #print(input_shape1)\n",
        "    #print(input_shape2)\n",
        "    for layer in res_img.layers:\n",
        "      layer.trainable = False\n",
        "    rgb_input = tf.keras.layers.Input(input_shape1)\n",
        "    #print(\"A\")\n",
        "    img_input = tf.keras.layers.Input(input_shape2)\n",
        "    rgb = tf.keras.layers.Conv2D(3, kernel_size=(33, 33), activation='relu')(rgb_input)\n",
        "    scale_layer = tf.keras.layers.Rescaling(scale=1 / 255, offset=-1)\n",
        "    rgb = scale_layer(rgb)  \n",
        "    #img = tf.keras.layers.Conv2D(8, kernel_size=(33, 33), activation='relu')(img_input)\n",
        "    #print(rgb)\n",
        "    tab_input = tf.keras.layers.Input(input_shape3)\n",
        "    #print(\"B\")\n",
        "    rgb = res_img(rgb, training = False)\n",
        "    #rgb = tf.keras.layers.GlobalAveragePooling2D()(rgb)\n",
        "    #rgb = tf.keras.layers.Conv2D(3, (3,3), strides = 32)(rgb)\n",
        "  #print(\"C\")\n",
        "    #rgb = tf.keras.layers.BatchNormalization()(rgb)\n",
        "    #rgb = tf.keras.layers.Activation('relu')(rgb)\n",
        "    #rgb = tf.keras.layers.Conv2D(3, (9,9), strides = 2, padding = \"same\")(rgb)\n",
        "  #print(\"C\")\n",
        "    #rgb = tf.keras.layers.BatchNormalization()(rgb)\n",
        "    #rgb = tf.keras.layers.Activation('relu')(rgb)\n",
        "    #rgb = tf.keras.layers.Conv2D(3, (9,9), strides = 2, padding = \"same\")(rgb)\n",
        "  #print(\"C\")\n",
        "    #rgb = tf.keras.layers.BatchNormalization()(rgb)\n",
        "    #rgb = tf.keras.layers.Activation('relu')(rgb)\n",
        "    #print(\"C\")\n",
        "    #rgb = tf.keras.layers.Flatten()(rgb)\n",
        "    #print(\"D\")\n",
        "    img = img_input\n",
        "    #img = res_img(img_input, training = False)\n",
        "    rgb = tf.keras.layers.Dense(32, activation = 'relu')(rgb)\n",
        "    rgb = tf.keras.layers.Flatten()(rgb)\n",
        "\n",
        "    \n",
        "    #rgb = tf.keras.layers.GlobalAveragePooling2D()(rgb)\n",
        "    #rgb = tf.keras.layers.Dropout(0.2)(rgb)  # Regularize with dropout\n",
        "    \n",
        "\n",
        "    #img = tf.keras.layers.GlobalAveragePooling2D()(img)\n",
        "    #img = tf.keras.layers.Dropout(0.2)(img)  # Regularize with dropout\n",
        "\n",
        "    #z = tf.keras.layers.Concatenate(axis=1)([rgb, img])\n",
        "\n",
        "    classifier = tf.keras.layers.Dense(num_classes, activation='softmax')(rgb)\n",
        "\n",
        "  #classifier = tf.keras.layers.Dense(num_classes, name='outputs', activation='softmax')(x)\n",
        "    model = tf.keras.models.Model(inputs = [rgb_input, img_input, tab_input], outputs = classifier, name = \"pre-t-all-imag\")\n",
        "    #for layer in res_img.layers:\n",
        "    #    layer.trainable=False\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tJTm-MQji7EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Multimodal"
      ],
      "metadata": {
        "id": "pnqU0MZduAAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_modal_cnn():\n",
        "    #with mirrored_strategy.scope():\n",
        "\n",
        "        # Inputs\n",
        "        patch_input = tf.keras.layers.Input(shape=(256, 256, 6), dtype='float32')\n",
        "        tabular_input = tf.keras.layers.Input(shape=(29), dtype='float32')  \n",
        "\n",
        "        # Augment data\n",
        "    #     augmented = data_augmentation_for_visualization(patch_input)\n",
        "\n",
        "        # From Scratch model\n",
        "        x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(patch_input)\n",
        "        x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "        x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu')(x)\n",
        "        x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "        x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu')(x)\n",
        "        x = tf.keras.layers.MaxPooling2D((1, 1), padding='same')(x)\n",
        "        x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "        x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "        x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "        x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)      \n",
        "\n",
        "        # Add Dense layers for images\n",
        "        x = tf.keras.layers.Flatten()(x)\n",
        "        x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "        # Add Dense layers for Tabular data\n",
        "        y = tf.keras.layers.Dense(512, activation='relu')(tabular_input)\n",
        "        y = tf.keras.layers.Dense(256, activation='relu')(y)\n",
        "\n",
        "        # Concatenate Image and tabular weights\n",
        "        z = tf.keras.layers.Concatenate(axis=1)([x, y])\n",
        "\n",
        "        # Add Classification Head\n",
        "        z = tf.keras.layers.Dense(128, activation='relu')(z)\n",
        "        classifier = tf.keras.layers.Dense(num_classes, name='outputs', activation='softmax')(z)\n",
        "\n",
        "        # Define inputs and outputs\n",
        "        model = tf.keras.Model(inputs=[patch_input, tabular_input], outputs=classifier)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tfa.optimizers.AdamW(learning_rate=0.001, \n",
        "                                         weight_decay=weight_decay_rate)\n",
        "        # Compile model\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=[\n",
        "                          tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "                          tf.keras.metrics.SparseTopKCategoricalAccuracy(10, name=\"top-10-accuracy\")\n",
        "                      ]\n",
        "                      )"
      ],
      "metadata": {
        "id": "V229zpGqt_i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For Images (Includes RGB, near IR, landcover, altitude) + Tabular (environmental vectors)"
      ],
      "metadata": {
        "id": "lzjAB49wo0hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCHSIZE = 128\n",
        "train_generator = Environmental_Patches_Generator(obs_id_train, y_train, BATCHSIZE)\n",
        "print(\"HERRE\")\n",
        "val_generator = Environmental_Patches_Generator(obs_id_val, y_val, BATCHSIZE)\n",
        "\n",
        "# converting our train dataset to tf.data.Dataset\n",
        "tf_train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: train_generator ,  # Our generator \n",
        "    output_types = ({'input_1': tf.float32 , 'input_2': tf.float32}, tf.float32) , # How we're expecting our output dtype\n",
        "#    output_shapes = ({'input_1': [BATCH_SIZE, 256 , 256, 6], 'input_2': [BATCH_SIZE, 29]} , [BATCH_SIZE, ]) # How we're expecting our output shape\n",
        ")\n",
        "\n",
        "tf_val_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: val_generator , \n",
        "    output_types = ({'input_1': tf.float32 , 'input_2': tf.float32}, tf.float32),\n",
        "#    output_shapes = ({'input_1': [BATCH_SIZE, 256 , 256, 6], 'input_2': [BATCH_SIZE, 29]} , [BATCH_SIZE, ]) \n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "num_classes = len(set(df_obs['species_id']))\n",
        "\n",
        "input_shape1 = (256, 256, 6)\n",
        "input_shape2 = (27)\n",
        "model = my_res(input_shape1, input_shape2, num_classes)\n",
        "# Compile model\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[\n",
        "                  tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "                  tf.keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\")\n",
        "              ]\n",
        "              )\n"
      ],
      "metadata": {
        "id": "NHSmCz5DozSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "661d8389-165d-456c-8173-b49440c3e2e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HERRE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-Trained All Images Code"
      ],
      "metadata": {
        "id": "qJc4ztWEps5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCHSIZE = 64\n",
        "train_generator = Pretrained_Patches_Generator(obs_id_train, y_train, BATCHSIZE)\n",
        "print(\"HERRE\")\n",
        "val_generator = Pretrained_Patches_Generator(obs_id_val, y_val, BATCHSIZE)\n",
        "\n",
        "# converting our train dataset to tf.data.Dataset\n",
        "#tf_train_dataset = tf.data.Dataset.from_generator(\n",
        "#    lambda: train_generator ,  # Our generator \n",
        "#    output_types = ({'input_1': tf.float32 , 'input_2': tf.float32, 'input_3': tf.float32}, tf.float32) , # How we're expecting our output dtype\n",
        "#    output_shapes = ({'input_1': [BATCH_SIZE, 256 , 256, 6], 'input_2': [BATCH_SIZE, 29]} , [BATCH_SIZE, ]) # How we're expecting our output shape\n",
        "#)\n",
        "\n",
        "#tf_val_dataset = tf.data.Dataset.from_generator(\n",
        "#    lambda: val_generator , \n",
        "#    output_types = ({'input_1': tf.float32 , 'input_2': tf.float32, 'input_3': tf.float32}, tf.float32),\n",
        "#    output_shapes = ({'input_1': [BATCH_SIZE, 256 , 256, 6], 'input_2': [BATCH_SIZE, 29]} , [BATCH_SIZE, ]) \n",
        "#)\n",
        "\n",
        "\n",
        "#PROBLEM\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "num_classes = len(set(df_obs['species_id']))\n",
        "#num_classes = 10\n",
        "input_shape1 = (256, 256, 3)\n",
        "input_shape2 = (256, 256, 3)\n",
        "input_shape3 = (27)\n",
        "model1 = transfer_learning(input_shape1, input_shape2, input_shape3, num_classes)\n",
        "# Compile model\n",
        "model1.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[\n",
        "                  tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "                  tf.keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\")\n",
        "              ]\n",
        "              )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swIy3cu2prs8",
        "outputId": "9b072f57-4166-4cdb-9ffd-50b6a0db6210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HERRE\n",
            "Model: \"pre-t-all-imag\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_226 (InputLayer)         [(None, 256, 256, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_91 (Conv2D)             (None, 224, 224, 3)  9804        ['input_226[0][0]']              \n",
            "                                                                                                  \n",
            " rescaling_9 (Rescaling)        (None, 224, 224, 3)  0           ['conv2d_91[0][0]']              \n",
            "                                                                                                  \n",
            " resnet50 (Functional)          (None, 7, 7, 2048)   23587712    ['rescaling_9[0][0]']            \n",
            "                                                                                                  \n",
            " dense_53 (Dense)               (None, 7, 7, 32)     65568       ['resnet50[0][0]']               \n",
            "                                                                                                  \n",
            " flatten_63 (Flatten)           (None, 1568)         0           ['dense_53[0][0]']               \n",
            "                                                                                                  \n",
            " input_227 (InputLayer)         [(None, 256, 256, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " input_228 (InputLayer)         [(None, 27)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_54 (Dense)               (None, 17037)        26731053    ['flatten_63[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 50,394,137\n",
            "Trainable params: 26,806,425\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For Environmental Vectors"
      ],
      "metadata": {
        "id": "bbeabZ8QHA-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_env = pd.read_csv(\"./geolifeclef-2022-lifeclef-2022-fgvc9/pre-extracted/environmental_vectors.csv\", sep=\";\", index_col=\"observation_id\")\n",
        "\n",
        "X_train = df_env.loc[obs_id_train].values\n",
        "X_val = df_env.loc[obs_id_val].values\n",
        "X_test = df_env.loc[obs_id_test].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKCXEDZVHARU",
        "outputId": "49acb8f6-623a-49b5-d30b-bcdb416c65b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extractor = PatchExtractor(\"./geolifeclef-2022-lifeclef-2022-fgvc9/pre-extracted/rasters\", size=256)\n",
        "extractor.add_all_bioclimatic_rasters()\n"
      ],
      "metadata": {
        "id": "9wLCbjkhKQEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run Model"
      ],
      "metadata": {
        "id": "YUpKMJCQJpuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.001, patience=10, \n",
        "                                            verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
        "train_results = model.fit(\n",
        "    #train_ds,\n",
        "    train_generator,\n",
        "    validation_data = val_generator,\n",
        "    #tf_train_dataset,\n",
        "    #validation_data= val_ds,\n",
        "    #validation_data = tf_val_dataset,\n",
        "    epochs = 100,\n",
        "    callbacks=[early_stop])\n",
        "#)\n",
        "# callbacks=[early_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "cpfxWC7F4PQK",
        "outputId": "7e716e1b-03cf-4181-eb3c-e3b82f633a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-9392e4e20f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.001, patience=10, \n\u001b[1;32m      2\u001b[0m                                             verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n\u001b[0;32m----> 3\u001b[0;31m train_results = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#train_ds,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "04Xc4Ow-qmSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Model Pretrained"
      ],
      "metadata": {
        "id": "zgsusc2oqpEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.001, patience=10, \n",
        "                                            verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
        "train_results = model1.fit(\n",
        "    #train_ds,\n",
        "    train_generator,\n",
        "    validation_data = val_generator,\n",
        "    #tf_train_dataset,\n",
        "    #validation_data= val_ds,\n",
        "    #validation_data = tf_val_dataset,\n",
        "    epochs = 100,\n",
        "    callbacks=[early_stop])\n",
        "#)\n",
        "# callbacks=[early_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "EjAk6p47qrNf",
        "outputId": "d65a793e-cb7e-4a1f-b852-83330f7c839c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  266/24804 [..............................] - ETA: 3:38:42 - loss: 8.7786 - accuracy: 0.0031 - top-5-accuracy: 0.0150"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-168-cbd1991fbe8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#validation_data = tf_val_dataset,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     callbacks=[early_stop])\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# callbacks=[early_stop])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1219\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \"\"\"\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \"\"\"\n\u001b[1;32m   1148\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Submission"
      ],
      "metadata": {
        "id": "Ks-Hm0K6N1rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = Test_Patches_Generator(obs_id_test, BATCHSIZE)\n",
        "\n",
        "tf_test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: test_generator,  # Our generator \n",
        "    output_types = (tf.float32 , tf.float32), # How we're expecting our output dtype\n",
        "    #output_shapes = ([BATCHSIZE, 256 , 256, 3] , [BATCHSIZE, ]) # How we're expecting our output shape\n",
        "    #output_shapes = (tf.TensorShape(features_shape), [BATCHSIZE, ]),\n",
        ")"
      ],
      "metadata": {
        "id": "Nj_vHzB9TQCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "SUBMISSION_PATH = Path(\"submissions\")\n",
        "os.makedirs(SUBMISSION_PATH, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "fqbiABH4_i6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from GLC.submission import generate_submission_file\n",
        "\n",
        "\n",
        "n_test = len(df_obs_test)\n",
        "s_pred = model.predict_generator(test_generator)\n",
        "\n",
        "# Generate the submission file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "0Kc4vUAlAZoC",
        "outputId": "a9ba16ce-2f0d-407f-af9b-649b52cd6e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-4320672829e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Generate the submission file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgenerate_submission_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSUBMISSION_PATH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"constant_top_30_most_present_species_baseline.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_obs_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'SUBMISSION_PATH' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(s_pred[11])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "rIJpYev9P9U8",
        "outputId": "b274faee-1a1c-4530-8ebd-84be45916cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b4c2f3e027fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 's_pred' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_submission_file(\"./submissions/new\", df_obs_test.index, s_pred)"
      ],
      "metadata": {
        "id": "Wa6Qr79tPC4E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Kennedy_subset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}