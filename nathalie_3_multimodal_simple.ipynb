{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, GlobalAveragePooling2D, Conv2D, Dense, AveragePooling2D, \\\n",
    "BatchNormalization, Normalization, Dropout, Flatten, Lambda, Input, Activation, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import schedules, SGD\n",
    "from tensorflow.keras.callbacks import Callback, LambdaCallback\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "#import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import scale\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import copy\n",
    "import threading\n",
    "import opendatasets as od\n",
    "import tempfile\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "%pylab inline --no-import-all\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from GLC.data_loading.common import load_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only uncomment if you don't have the dataset stored on disk yet\n",
    "# -> have your kaggle user credentials ready\n",
    "#data = od.download(\"https://www.kaggle.com/competitions/geolifeclef-2022-lifeclef-2022-fgvc9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path to competition dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this path to adapt to where you downloaded the data\n",
    "DATA_PATH = Path(\"./geolifeclef-2022-lifeclef-2022-fgvc9/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following two commands to verify that the data path is set correctly. They should print folder and file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -L $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls $DATA_PATH/observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the observation ids of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Dataset ###\n",
    "# let's load the data from file\n",
    "df_obs_fr = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "df_obs = pd.concat((df_obs_fr, df_obs_us))\n",
    "\n",
    "print(\"Number of observations for training: {}\".format(len(df_obs)))\n",
    "\n",
    "# let's have a look at the data\n",
    "df_obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the observation ids of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test Dataset ###\n",
    "df_obs_fr_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "df_obs_test = pd.concat((df_obs_fr_test, df_obs_us_test))\n",
    "\n",
    "print(\"Number of observations for testing: {}\".format(len(df_obs_test)))\n",
    "\n",
    "df_obs_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load suggested landcover alignment (only relevant if you're using landcover data later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_suggested_landcover_alignment = pd.read_csv(DATA_PATH / \"metadata\" / \"landcover_suggested_alignment.csv\", sep=\";\")\n",
    "print(df_suggested_landcover_alignment.head())\n",
    "landcover_mapping = df_suggested_landcover_alignment[\"suggested_landcover_code\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Split Labels\n",
    "Retrieve the train/val split provided, and load the labels of the train and val set elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_id_train = df_obs.index[df_obs[\"subset\"] == \"train\"].values\n",
    "obs_id_val = df_obs.index[df_obs[\"subset\"] == \"val\"].values\n",
    "\n",
    "y_train = df_obs.loc[obs_id_train][\"species_id\"].values\n",
    "y_val = df_obs.loc[obs_id_val][\"species_id\"].values\n",
    "\n",
    "n_val = len(obs_id_val)\n",
    "print(\"Training set size: {} ({:.1%} of train observations)\".format(len(y_train), len(y_train) / len(df_obs)))\n",
    "print(\"Validation set size: {} ({:.1%} of train observations)\".format(n_val, n_val / len(df_obs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's get the environmental vectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "#imp = SimpleImputer(\n",
    "#    missing_values=np.nan,\n",
    "#    strategy=\"constant\",\n",
    "#    fill_value=np.finfo(np.float32).min,\n",
    "#)\n",
    "df_env = pd.read_csv(\"./geolifeclef-2022-lifeclef-2022-fgvc9/pre-extracted/environmental_vectors.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "a  = df_env.loc(0)\n",
    "columnsNamesArr = df_env.columns.values\n",
    "#print(columnsNamesArr)\n",
    "rowNames = df_env.index.values\n",
    "#print(rowNames)\n",
    "my_imputer = SimpleImputer()\n",
    "#print(df_env.shape)\n",
    "#print(my_imputer.fit_transform(df_env)[0])\n",
    "\n",
    "df_env = pd.DataFrame(my_imputer.fit_transform(df_env))\n",
    "df_env.set_axis(rowNames, axis='index', inplace = True)\n",
    "df_env.set_axis(columnsNamesArr, axis='columns', inplace = True)\n",
    "#print(df_env.shape)\n",
    "\n",
    "df_env.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain train, val and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Kennedy's Train, Val and Test Split ###\n",
    "# CHOOSE SUBSET FOR 30 Labels\n",
    "label_amount = 30\n",
    "\n",
    "import random\n",
    "subset_size = 0\n",
    "obs_list = list()\n",
    "obs_test_list = list()\n",
    "import numpy as np\n",
    "# iterate over a subset of the labels\n",
    "m = 0\n",
    "for y in (np.unique(y_train)[:]):\n",
    "    #print(\"in\")\n",
    "    # for each label, retrieve all corresponding observation ids\n",
    "    obs = df_obs.index[(df_obs[\"species_id\"] == y)]\n",
    "   \n",
    "    #print(len(obs))\n",
    "    #print(counter)\n",
    "    #print(obs)\n",
    "    #print(len(obs))\n",
    "    if (len(obs) >= 2000 and len(obs) <= 3000):\n",
    "      t = set(df_obs.index[(df_obs[\"species_id\"] == y) & (df_obs[\"subset\"] == \"train\")].values)\n",
    "      m += len(t)\n",
    "      ten_perc = int(len(t)/10)\n",
    "      random.seed(3)\n",
    "      test = random.sample(t, ten_perc)\n",
    "      train = t-set(test)\n",
    "      #print(train)\n",
    "      #print(\"here\")\n",
    "      obs_test_list.append(list(test))\n",
    "      obs_list.append(list(train))\n",
    "      subset_size += 1\n",
    "    if (subset_size >= label_amount):\n",
    "      #print(\"break\")\n",
    "      break\n",
    "print(m)   \n",
    "# we now have a numpy array of all observation ids corresponding to this subset of labels\n",
    "obs_id_train = np.concatenate(obs_list)\n",
    "obs_id_test = np.concatenate(obs_test_list)\n",
    "gps_train = np.concatenate((df_obs.loc[obs_id_train][\"latitude\"].values, df_obs.loc[obs_id_train][\"longitude\"].values))\n",
    "# obtain the labels in the right order \n",
    "y_train = df_obs.loc[obs_id_train][\"species_id\"].values\n",
    "y_test = df_obs.loc[obs_id_test][\"species_id\"].values\n",
    "print(y_train.size)\n",
    "\n",
    "print()\n",
    "print(y_test.size)\n",
    "print(y_train[2])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_size = 8\n",
    "obs_list_1 = list()\n",
    "print(y_val.size)\n",
    "\n",
    "# iterate over a subset of the labels\n",
    "counter = 0\n",
    "print(y_val[0])\n",
    "for y in (np.unique(y_val)[:]):\n",
    "    # for each label, retrieve all corresponding observation ids\n",
    "    if (y in y_train):\n",
    "      #obs = df_obs.index[df_obs[\"species_id\"] == y].values\n",
    "      v = df_obs.index[(df_obs[\"species_id\"] == y) & (df_obs[\"subset\"] == \"val\")].values\n",
    "      obs_list_1.append(v)\n",
    "# we now have a numpy array of all observation ids corresponding to this subset of labels\n",
    "obs_id_val = np.concatenate(obs_list_1)\n",
    "\n",
    "# obtain the labels in the right order \n",
    "y_val = df_obs.loc[obs_id_val][\"species_id\"].values\n",
    "gps_val = np.concatenate((df_obs.loc[obs_id_val][\"latitude\"].values, df_obs.loc[obs_id_val][\"longitude\"].values))\n",
    "\n",
    "print(y_val.size)\n",
    "print(obs_id_val == obs_id_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dict_count = defaultdict(lambda: 0)\n",
    "for y in y_train:\n",
    "    train_dict_count[y] += 1\n",
    "    \n",
    "print(\"training: \")\n",
    "for key, value in train_dict_count.items():\n",
    "    print(\"label {:>4}: {:.2f}%\".format(key, value/len(y_train)))\n",
    "\n",
    "print()\n",
    "\n",
    "val_dict_count = defaultdict(lambda: 0)\n",
    "for y in y_val:\n",
    "    val_dict_count[y] += 1\n",
    "    \n",
    "print(\"validation: \")\n",
    "for key, value in val_dict_count.items():\n",
    "    print(\"label {:>4}: {:.2f}%\".format(key, value/len(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.unique(y_train) == np.unique(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remap the labels s.t. they go from 0 to n-1\n",
    "(NAN fix is here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map s.t. the labels will go from 0 to n-1\n",
    "map_labels = dict()\n",
    "i = 0\n",
    "for l in np.unique(y_train):\n",
    "    map_labels[l] = i\n",
    "    i+=1\n",
    "print(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the map to the training labels\n",
    "y_train_normalized = np.zeros(np.shape(y_train), dtype='int64')\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    y_train_normalized[i] = map_labels[y_train[i]]\n",
    "    \n",
    "# shuffle together\n",
    "obs_id_train, y_train = shuffle(obs_id_train, y_train_normalized)\n",
    "\n",
    "no_output_neurons = len(np.unique(y_train))\n",
    "print(\"# output neurons: \", no_output_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the map to the validation labels\n",
    "y_val_normalized = np.zeros(np.shape(y_val), dtype='int64')\n",
    "\n",
    "for i in range(len(y_val)):\n",
    "    y_val_normalized[i] = map_labels[y_val[i]]\n",
    "    \n",
    "# shuffle together\n",
    "obs_id_val, y_val = shuffle(obs_id_val, y_val_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write input pipeline to load batches as we train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Generator\n",
    "Since dataset is too large to load it all into memory once, we need to load it from disk in batches as we train. Such a generator can later be passed into model.fit() instead of a train and/or validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches_Generator(tf.keras.utils.Sequence) :\n",
    "  \n",
    "    def __init__(self, obs_ids, labels, batch_size) :\n",
    "        self.obs_ids = obs_ids\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # to make the generator thread safe \n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.obs_ids) / float(self.batch_size))).astype(int)\n",
    "  \n",
    "    # returns one batch\n",
    "    def __getitem__(self, idx) :\n",
    "        X_batch = list()\n",
    "        y_batch = list()\n",
    "\n",
    "        for i in range(idx * self.batch_size, (idx+1) * self.batch_size):\n",
    "            if i >= len(self.obs_ids): break\n",
    "            \n",
    "            patch = load_patch(self.obs_ids[i], DATA_PATH, data='rgb')\n",
    "            X_batch.append(patch[0])\n",
    "            y_batch.append(self.labels[i])\n",
    "\n",
    "        with self.lock:\n",
    "            return np.asarray(X_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class Environmental_Patches_Generator(tf.keras.utils.Sequence) :\n",
    "  \n",
    "    def __init__(self, obs_ids, labels, batch_size) :\n",
    "        self.obs_ids = obs_ids\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        #self.gps = gps\n",
    "        #self.extractor = extractor\n",
    "        #print(\"INIT\")\n",
    "        # to make the generator thread safe \n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.obs_ids) / float(self.batch_size))).astype(int)\n",
    "  \n",
    "    # returns one batch\n",
    "    def __getitem__(self, idx) :\n",
    "        X_batch = list()\n",
    "        y_batch = list()\n",
    "        X_env_batch = list()\n",
    "\n",
    "        #print(\"ONE BATCH\")\n",
    "        for i in range(idx * self.batch_size, (idx+1) * self.batch_size):\n",
    "            if i >= len(self.obs_ids): break\n",
    "            \n",
    "            rgb, near_ir, landcover, altitude = load_patch(self.obs_ids[i], DATA_PATH, data='all')\n",
    "            ni = near_ir.reshape(256, 256, 1)\n",
    "            lc = landcover.reshape(256, 256, 1)\n",
    "            alt = altitude.reshape(256, 256, 1)\n",
    "\n",
    "            patch = np.concatenate((rgb, ni, lc, alt), axis=2)\n",
    "\n",
    "            #cs = MinMaxScaler()\n",
    "            #print(\"PATCH GENERATOR\")\n",
    "            #print((df_env.loc[self.obs_ids[i]].values).shape)\n",
    "            #print(cs.fit_transform(df_env.loc[self.obs_ids[i]].values).shape)\n",
    "            #k = input()\n",
    "            X_env_batch.append(df_env.loc[self.obs_ids[i]].values)\n",
    "            #X_env_batch.append(df_env[self.obs_ids[i], :])\n",
    "            #X_env_batch.append(cs.fit_transform(df_env.loc[self.obs_ids[i]].values.reshape(-1,1)))\n",
    "            X_batch.append(patch)\n",
    "            y_batch.append(self.labels[i])\n",
    "\n",
    "        with self.lock:\n",
    "            \n",
    "            #return {'input_1': np.asarray(X_batch), 'input_2': np.asarray(X_env_batch)}, np.asarray(np.array(y_batch))\n",
    "            #return np.asarray(X_batch), np.array(y_batch)\n",
    "            return (np.asarray(X_batch), np.asarray(X_env_batch)), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Simple Neural Network\n",
    "Let's create a first neural network as a baseline to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for distributed training (that is, using multiple GPUs for data parallelization)\n",
    "# # https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a 10 layer ReLU model of width 2\n",
    "def simple_model(input_shape, learning_rate=0.0001, output_neurons=30):\n",
    "    random.seed(4)\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # 1. Preprocessing\n",
    "    # rescale inputs\n",
    "    model.add(tf.keras.layers.Rescaling(1./255))\n",
    "\n",
    "    # 2. Convolutional Layers\n",
    "    model.add(Conv2D(32, kernel_size=5, activation='relu', input_shape=input_shape, padding='same'))\n",
    "    #model.add(AveragePooling2D())\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=5, activation='relu', padding='same'))\n",
    "    #model.add(AveragePooling2D())\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=5, activation='relu', padding='same'))\n",
    "    \n",
    "    # from convolutional layers to dense layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    # 3. Dense Layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    \n",
    "    # 4. Output Layer\n",
    "    model.add(Dense(output_neurons, activation='softmax'))\n",
    "    \n",
    "    # compire the model\n",
    "    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_simple_cnn(input_shape1=(256, 256, 6), input_shape2=(29), learning_rate=0.001, output_neurons=30):\n",
    "    #with mirrored_strategy.scope():\n",
    "\n",
    "        # Inputs\n",
    "        patch_input = tf.keras.layers.Input(shape=input_shape1, dtype='float32')\n",
    "        tabular_input = tf.keras.layers.Input(shape=input_shape2, dtype='float32')  \n",
    "\n",
    "        # Augment data\n",
    "        # augmented = data_augmentation_for_visualization(patch_input)\n",
    "        x = tf.keras.layers.Rescaling(1./255)(patch_input)\n",
    "        x = tf.keras.layers.RandomContrast(factor=0.1)(x)\n",
    "        x = tf.keras.layers.RandomCrop(input_shape[0], input_shape[1])(x)\n",
    "\n",
    "        # From Scratch model\n",
    "        x = tf.keras.layers.Conv2D(32, kernel_size=5, activation='relu')(x)\n",
    "        x = tf.keras.layers.Conv2D(64, kernel_size=5, activation='relu')(x)\n",
    "        x = tf.keras.layers.Conv2D(128, kernel_size=5, activation='relu')(x)\n",
    "\n",
    "        # Add Dense layers for images\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "        # Add Dense layers for Tabular data\n",
    "        y = tf.keras.layers.Dense(64, activation='relu')(tabular_input)\n",
    "        y = tf.keras.layers.Dense(128, activation='relu')(y)\n",
    "\n",
    "        # Concatenate Image and tabular weights\n",
    "        z = tf.keras.layers.Concatenate(axis=1)([x, y])\n",
    "\n",
    "        # Add Classification Head\n",
    "        z = tf.keras.layers.Dense(128, activation='relu')(z)\n",
    "        classifier = tf.keras.layers.Dense(output_neurons, name='outputs', activation='softmax')(z)\n",
    "\n",
    "        # Define inputs and outputs\n",
    "        model = tf.keras.Model(inputs=[patch_input, tabular_input], outputs=classifier)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                      metrics=[\n",
    "                          tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "                          tf.keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\")\n",
    "                      ]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings used when parallelizing the I/O Pipeline in model.fit().\n",
    "\n",
    "I used the setting values in the cell below for a high-cpu vm on GCP with the following specs:\n",
    "- machine type: n1-highcpu-96 (96 CPU cores)\n",
    "- vCPUs to core ratio: 2 vCPUs per core (making a theoretical max value for num_threads of 96 * 2 = 192)\n",
    "- 4 x NVIDIA Tesla T4 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to tune the learning rate accordingly.\n",
    "BATCHSIZE = 64\n",
    "\n",
    "# The maximun value for num_threads is dependent on amount of CPU cores:\n",
    "# amount of CPU cores * vCPUs to core ratio = theoretical max of NUM_THREADS\n",
    "NUM_THREADS = 11\n",
    "\n",
    "# The more batches we prefetch, the less idle the GPUs will be. \n",
    "# To check GPU usage:\n",
    "# 1. Run nvidia-smi -l 1 from the terminal to monitor the GPU usage during training. \n",
    "# 2. Try to get close to 100% for all GPUs by adjusting the value below (and the two above). Due to the overhead\n",
    "#    from tf.distribute.MirroredStrategy(), you won't be able to consistently get 100% for all GPUs. But try to \n",
    "#    get close.\n",
    "# 3. Be aware that RAM limits the amount of batches you can prefetch.\n",
    "PRE_FETCH_NUM_BATCHES = int(NUM_THREADS * 30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators\n",
    "Create generators that will read training / validation data from disk during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_generator = Patches_Generator(obs_id_train, y_train, BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_generator = Patches_Generator(obs_id_val, y_val, BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = Environmental_Patches_Generator(obs_id_train, y_train, BATCHSIZE)\n",
    "val_generator = Environmental_Patches_Generator(obs_id_val, y_val, BATCHSIZE)\n",
    "\n",
    "# converting our train dataset to tf.data.Dataset\n",
    "tf_train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_generator ,  # Our generator \n",
    "    output_types = ({'input_1': tf.float32 , 'input_2': tf.float32}, tf.float32) , # How we're expecting our output dtype\n",
    "#    output_shapes = ({'input_1': [BATCH_SIZE, 256 , 256, 6], 'input_2': [BATCH_SIZE, 29]} , [BATCH_SIZE, ]) # How we're expecting our output shape\n",
    ")\n",
    "\n",
    "tf_val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: val_generator , \n",
    "    output_types = ({'input_1': tf.float32 , 'input_2': tf.float32}, tf.float32),\n",
    "#    output_shapes = ({'input_1': [BATCH_SIZE, 256 , 256, 6], 'input_2': [BATCH_SIZE, 29]} , [BATCH_SIZE, ]) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network\n",
    "#model = simple_model((256, 256, 3), learning_rate=clr, output_neurons=len(np.unique(y_train)))\n",
    "#model = simple_model_with_gps([(256, 256, 3), np.shape(gps)], learning_rate=clr, output_neurons=len(np.unique(y_train)))\n",
    "\n",
    "#model = simple_model((256, 256, 3), output_neurons = no_output_neurons, learning_rate=0.00001)\n",
    "\n",
    "# create the model\n",
    "input_shape1 = (256, 256, 6)\n",
    "input_shape2 = (27) # maybe (29) ?\n",
    "\n",
    "model = multi_modal_simple_cnn(input_shape1, input_shape2, learning_rate=0.0001, output_neurons=output_neurons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, \n",
    "                                              verbose=0, mode='auto', baseline=None, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(tf_train_dataset, epochs=100, callbacks=[early_stop], #steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    validation_data=tf_val_dataset,\n",
    "                    # for parallelization of reading from disk (I/O) pipeline\n",
    "                    max_queue_size=PRE_FETCH_NUM_BATCHES, workers=NUM_THREADS, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save('simple_simplecnn_val_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
