{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'GLC'...\n",
      "remote: Enumerating objects: 383, done.\u001b[K\n",
      "remote: Counting objects: 100% (228/228), done.\u001b[K\n",
      "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
      "remote: Total 383 (delta 119), reused 170 (delta 63), pack-reused 155\u001b[K\n",
      "Receiving objects: 100% (383/383), 10.57 MiB | 35.02 MiB/s, done.\n",
      "Resolving deltas: 100% (205/205), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf GLC\n",
    "!git clone https://github.com/maximiliense/GLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-th2cd152 because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, GlobalAveragePooling2D, Conv2D, Dense, AveragePooling2D, \\\n",
    "BatchNormalization, Normalization, Dropout, Flatten, Lambda, Input, Activation\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import schedules, SGD\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import backend as K\n",
    "#import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import scale\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import copy\n",
    "import threading\n",
    "import opendatasets as od\n",
    "\n",
    "%pylab inline --no-import-all\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from GLC.data_loading.common import load_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"username\":\"nathaliemh\",\"key\":\"cba80d1f619e96b238e4a95aa3017836\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = od.download(\"https://www.kaggle.com/competitions/geolifeclef-2022-lifeclef-2022-fgvc9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this path to adapt to where you downloaded the data\n",
    "DATA_PATH = Path(\"./geolifeclef-2022-lifeclef-2022-fgvc9/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmetadata\u001b[0m/      \u001b[01;34mpatches-fr\u001b[0m/  \u001b[01;34mpatches_sample\u001b[0m/  \u001b[01;34mrasters\u001b[0m/\r\n",
      "\u001b[01;34mobservations\u001b[0m/  \u001b[01;34mpatches-us\u001b[0m/  \u001b[01;34mpre-extracted\u001b[0m/   sample_submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls -L $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations_fr_test.csv   observations_us_test.csv\r\n",
      "observations_fr_train.csv  observations_us_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls $DATA_PATH/observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for training: 1627475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>species_id</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observation_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10561949</th>\n",
       "      <td>45.705116</td>\n",
       "      <td>1.424622</td>\n",
       "      <td>241</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10131188</th>\n",
       "      <td>45.146973</td>\n",
       "      <td>6.416794</td>\n",
       "      <td>101</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799362</th>\n",
       "      <td>46.783695</td>\n",
       "      <td>-2.072855</td>\n",
       "      <td>700</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10392536</th>\n",
       "      <td>48.604866</td>\n",
       "      <td>-2.825003</td>\n",
       "      <td>1456</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10335049</th>\n",
       "      <td>48.815567</td>\n",
       "      <td>-0.161431</td>\n",
       "      <td>157</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 latitude  longitude  species_id subset\n",
       "observation_id                                         \n",
       "10561949        45.705116   1.424622         241  train\n",
       "10131188        45.146973   6.416794         101  train\n",
       "10799362        46.783695  -2.072855         700  train\n",
       "10392536        48.604866  -2.825003        1456  train\n",
       "10335049        48.815567  -0.161431         157  train"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training Dataset ###\n",
    "# let's load the data from file\n",
    "df_obs_fr = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "df_obs = pd.concat((df_obs_fr, df_obs_us))\n",
    "\n",
    "print(\"Number of observations for training: {}\".format(len(df_obs)))\n",
    "\n",
    "# let's have a look at the data\n",
    "df_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for testing: 36421\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observation_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10782781</th>\n",
       "      <td>43.601788</td>\n",
       "      <td>6.940195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10364138</th>\n",
       "      <td>46.241711</td>\n",
       "      <td>0.683586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10692017</th>\n",
       "      <td>45.181095</td>\n",
       "      <td>1.533459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10222322</th>\n",
       "      <td>46.938450</td>\n",
       "      <td>5.298678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10241950</th>\n",
       "      <td>45.017433</td>\n",
       "      <td>0.960736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 latitude  longitude\n",
       "observation_id                      \n",
       "10782781        43.601788   6.940195\n",
       "10364138        46.241711   0.683586\n",
       "10692017        45.181095   1.533459\n",
       "10222322        46.938450   5.298678\n",
       "10241950        45.017433   0.960736"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test Dataset ###\n",
    "df_obs_fr_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "df_obs_test = pd.concat((df_obs_fr_test, df_obs_us_test))\n",
    "\n",
    "print(\"Number of observations for testing: {}\".format(len(df_obs_test)))\n",
    "\n",
    "df_obs_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   landcover_code  suggested_landcover_code suggested_landcover_label\n",
      "0               0                         0              Missing Data\n",
      "1               1                        11          Cultivated Crops\n",
      "2               2                        11          Cultivated Crops\n",
      "3               3                         6       Broad-leaved Forest\n",
      "4               4                         7         Coniferous Forest\n"
     ]
    }
   ],
   "source": [
    "df_suggested_landcover_alignment = pd.read_csv(DATA_PATH / \"metadata\" / \"landcover_suggested_alignment.csv\", sep=\";\")\n",
    "print(df_suggested_landcover_alignment.head())\n",
    "landcover_mapping = df_suggested_landcover_alignment[\"suggested_landcover_code\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Split Labels\n",
    "Retrieve the train/val split provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 529131 (32.5% of train observations)\n",
      "Validation set size: 40080 (2.5% of train observations)\n"
     ]
    }
   ],
   "source": [
    "obs_id_train = df_obs.index[df_obs[\"subset\"] == \"train\"].values\n",
    "obs_id_val = df_obs.index[df_obs[\"subset\"] == \"val\"].values\n",
    "\n",
    "obs_id_train = obs_id_train[:int(len(obs_id_train)/3)]\n",
    "\n",
    "y_train = df_obs.loc[obs_id_train][\"species_id\"].values\n",
    "y_val = df_obs.loc[obs_id_val][\"species_id\"].values\n",
    "\n",
    "n_val = len(obs_id_val)\n",
    "print(\"Training set size: {} ({:.1%} of train observations)\".format(len(y_train), len(y_train) / len(df_obs)))\n",
    "print(\"Validation set size: {} ({:.1%} of train observations)\".format(n_val, n_val / len(df_obs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529131\n"
     ]
    }
   ],
   "source": [
    "print(len(obs_id_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obs_id_train[int(1587395*0.99986):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4810"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====               ] 29.271%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(obs_id_train)):\n\u001b[1;32m     10\u001b[0m     obs_id \u001b[38;5;241m=\u001b[39m obs_id_train[i]\n\u001b[0;32m---> 12\u001b[0m     patch \u001b[38;5;241m=\u001b[39m \u001b[43mload_patch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlandcover_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlandcover_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrgb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#X_train.append(patch[0])\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     X_train[i] \u001b[38;5;241m=\u001b[39m patch[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~tf/dig/species-presence-prediction/GLC/data_loading/common.py:64\u001b[0m, in \u001b[0;36mload_patch\u001b[0;34m(observation_id, patches_path, data, landcover_mapping, return_arrays)\u001b[0m\n\u001b[1;32m     62\u001b[0m     rgb_patch \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(rgb_filename)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_arrays:\n\u001b[0;32m---> 64\u001b[0m         rgb_patch \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_patch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     patches\u001b[38;5;241m.\u001b[39mappend(rgb_patch)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnear_ir\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "File \u001b[0;32m~usr/local/lib/python3.8/dist-packages/PIL/Image.py:675\u001b[0m, in \u001b[0;36mImage.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    673\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 675\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ArrayData(new), dtype)\n",
      "File \u001b[0;32m~usr/local/lib/python3.8/dist-packages/PIL/Image.py:728\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    726\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 728\u001b[0m     l, s, d \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(d)\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load training dataset samples\n",
    "# factor = 1 means load full training dataset\n",
    "# factor = 100 means load 1/100 of the full dataset\n",
    "factor = 1\n",
    "last_print = time.time()\n",
    "\n",
    "#X_train = list() #np.array((np.shape(y_train), 256, 256, 3))\n",
    "X_train = np.zeros((len(obs_id_train), 256, 256, 3))\n",
    "for i in range(len(obs_id_train)):\n",
    "    obs_id = obs_id_train[i]\n",
    "    \n",
    "    patch = load_patch(obs_id, DATA_PATH, landcover_mapping=landcover_mapping, data='rgb')\n",
    "    #X_train.append(patch[0])\n",
    "    X_train[i] = patch[0]\n",
    "    \n",
    "    #percent_progress = len(X_train)/(len(y_train)/factor) * 100\n",
    "    percent_progress = i/(len(y_train)/factor) * 100\n",
    "    \n",
    "    if time.time() - last_print > 1:\n",
    "        sys.stdout.write('\\r')\n",
    "        # the exact output you're looking for:\n",
    "        sys.stdout.write(\"[%-20s] %.3f%%\" % ('='*int(percent_progress/5), percent_progress))\n",
    "        sys.stdout.flush()\n",
    "        last_print = time.time()\n",
    "        \n",
    "    #if len(X_train) >= (len(y_train)/factor):\n",
    "    #if percent_progress >= 99.98:\n",
    "    #    break\n",
    "print(\"done with loading\")\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "print(\"sample array shape: \", np.shape(X_train))\n",
    "print(\"label array shape: \", np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train[:]))\n",
    "train_ds = train_ds.batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write patches and labels to TFRecord\n",
    "This only needs to be done once to obtain a TFRecord file. This file then can be reused during future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.train.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with features that may be relevant.\n",
    "def image_tfexample(img, label):\n",
    "    image_shape = img.shape\n",
    "\n",
    "    feature = {\n",
    "      'height': _int64_feature(image_shape[0]),\n",
    "      'width': _int64_feature(image_shape[1]),\n",
    "      'depth': _int64_feature(image_shape[2]),\n",
    "      'label': _int64_feature(label),\n",
    "      'image_raw': _bytes_feature(img.tobytes()),\n",
    "    }\n",
    "    \n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the raw image files to a TFRecord file (record_file).\n",
    "# First, process all images into `tf.train.Example` messages.\n",
    "# Then, write to a `.tfrecords` file.   \n",
    "def writeToTFRecord(obs_ids, labels, record_file):\n",
    "    \n",
    "    counter = 0\n",
    "    last_print = time.time()\n",
    "    \n",
    "    options = tf.io.TFRecordOptions(compression_type = 'GZIP')\n",
    "    \n",
    "    with tf.io.TFRecordWriter(record_file, options=options) as writer:\n",
    "        for obs_id, label in zip(obs_ids, labels):\n",
    "            patch = load_patch(obs_id, DATA_PATH)\n",
    "\n",
    "            # convert single rgb image + label to tf example\n",
    "            tf_example = image_tfexample(patch[0], label)\n",
    "\n",
    "            # write to records file\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "            counter += 1\n",
    "\n",
    "            if time.time() - last_print > 1:\n",
    "                percent_progress = counter/len(labels) * 100\n",
    "                sys.stdout.write('\\r')\n",
    "                sys.stdout.write(\"[%-20s] %.3f%%\" % ('='*int(percent_progress/5), percent_progress))\n",
    "                sys.stdout.flush()\n",
    "                last_print = time.time()\n",
    "                \n",
    "            if counter > 10000:\n",
    "                break\n",
    "            \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether it has the same length as y_train\n",
    "np.shape(obs_id_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write training dataset samples to TFrecord\n",
    "train_record_amount = writeToTFRecord(obs_id_train, y_train, record_file = 'training.tfrecords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write training dataset samples to TFrecord\n",
    "val_record_amount = writeToTFRecord(obs_id_val, y_val, record_file = 'validation.tfrecords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing Around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to TFRecords file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with features that may be relevant.\n",
    "def image_tfexample(img, label):\n",
    "    image_shape = img.shape\n",
    "\n",
    "    feature = {\n",
    "      'height': _int64_feature(image_shape[0]),\n",
    "      'width': _int64_feature(image_shape[1]),\n",
    "      'depth': _int64_feature(image_shape[2]),\n",
    "      'label': _int64_feature(label),\n",
    "      'image_raw': _bytes_feature(img),\n",
    "    }\n",
    "    \n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the raw image files to `images.tfrecords`.\n",
    "# First, process all images into `tf.train.Example` messages.\n",
    "# Then, write to a `.tfrecords` file.\n",
    "\n",
    "record_file = 'X_train.tfrecords'\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    for img, label in zip(X_train, y_train):\n",
    "\n",
    "        # convert single image + label to tf example\n",
    "        img_string = np.array2string(img)\n",
    "        tf_example = image_tfexample(img, label)\n",
    "    \n",
    "        # write to records file\n",
    "        writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read first few images from records file to ensure it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "  # Parse the input tf.train.Example proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_records = tf.data.TFRecordDataset(record_file)\n",
    "parsed_image_records = image_records.map(_parse_image_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in parsed_image_records:\n",
    "  image_raw = record['image_raw'].numpy()\n",
    "  display.display(display.Image(data=image_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load entire dataset in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "dataset = tf.data.TFRecordDataset(record_file)\n",
    "\n",
    "# parse each instance\n",
    "dataset = dataset.map(_parse_image_function, num_parallel_calls=num_threads)\n",
    "\n",
    "# shuffle\n",
    "dataset = dataset.shuffle(buffer_size)\n",
    "\n",
    "# form batch and epoch\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.repeat(num_epoch)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "# get a batch\n",
    "x_batch, y_batch = self.iterator.get_next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write input pipeline to load batches as we train.\n",
    "The dataset is so large that it doesn't fit into memory. We have to load each batch as we're training.\n",
    "\n",
    "Plan (do this once for train and once for val):\n",
    "1. Store entire dataset as TFRecord.\n",
    "    - Convert each image to a TFExample as we're loading it from file. (Done)\n",
    "    - Store image in TFRecord of respecitve (train/val) dataset. (Done)\n",
    "2. Write custom generator which loads the batches from the TFRecord. \n",
    "    - https://medium.com/@mrgarg.rajat/training-on-large-datasets-that-dont-fit-in-memory-in-keras-60a974785d71\n",
    "3. Use custom generator with fit_generator.\n",
    "\n",
    "Implement performance speed-up: https://linuxtut.com/en/a7c31b08d2f76c886a92/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Generator\n",
    "Since dataset is too large to load it all into memory once, we need to load it from file in batches as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "  # Parse the input tf.train.Example proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRecords_Generator(tf.keras.utils.Sequence) :\n",
    "  \n",
    "    def __init__(self, record_filename, record_amount, batch_size) :\n",
    "        self.record_filename = record_filename\n",
    "        self.record_amount = record_amount\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self) :\n",
    "        return (np.ceil(self.record_amount / float(self.batch_size))).astype(int)\n",
    "  \n",
    "    def __getitem__(self, idx) :\n",
    "        record_dataset = tf.data.TFRecordDataset(self.record_filename)\n",
    "        parsed_image_records = record_dataset.map(_parse_image_function)\n",
    "        \n",
    "        ds2 = tf.data.TFRecordDataset(self.record_filename) \\\n",
    "          .batch(batch_size) \\\n",
    "          .apply(tf.data.experimental.parse_example_dataset(image_feature_description)) \\\n",
    "          .map(dict2tuple)\n",
    "\n",
    "        #batch_records = \n",
    "\n",
    "        batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "        return np.array([\n",
    "                resize(imread('/content/all_images/' + str(file_name)), (80, 80, 3))\n",
    "                   for file_name in batch_x])/255.0, np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raw_Generator(tf.keras.utils.Sequence) :\n",
    "  \n",
    "    def __init__(self, obs_ids, labels, batch_size) :\n",
    "        self.obs_ids = obs_ids\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.obs_ids) / float(self.batch_size))).astype(int)\n",
    "  \n",
    "    def __getitem__(self, idx) :\n",
    "        X_batch = list()\n",
    "        y_batch = list()\n",
    "\n",
    "        for i in range(idx * self.batch_size, (idx+1) * self.batch_size):\n",
    "            patch = load_patch(self.obs_ids[i], DATA_PATH)\n",
    "            X_batch.append(patch[0])\n",
    "            y_batch.append(self.labels[i])\n",
    "\n",
    "        return np.asarray(X_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Simple Neural Network\n",
    "Let's create a first neural network as a baseline to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for distributed training\n",
    "# https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a 10 layer ReLU model of width 2\n",
    "def simple_model(input_shape):\n",
    "    \n",
    "    # for distributed training\n",
    "    with mirrored_strategy.scope():\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(Conv2D(6, 5, activation='tanh', input_shape=input_shape))\n",
    "        model.add(AveragePooling2D(2))\n",
    "        model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "        model.add(Conv2D(16, 5, activation='tanh'))\n",
    "        model.add(AveragePooling2D(2))\n",
    "        model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "        model.add(Conv2D(120, 5, activation='tanh'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(84, activation='tanh'))\n",
    "        model.add(Dense(17031, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    \"\"\"model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # 1. Preprocessing\n",
    "    # rescale inputs\n",
    "    model.add(tf.keras.layers.Rescaling(1./255))\n",
    "    \n",
    "    # 2. Convolutional Layers\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=input_shape, padding='same'))\n",
    "    model.add(AveragePooling2D())\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(AveragePooling2D())\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(AveragePooling2D())\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    \n",
    "    # from convolutional layers to dense layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    # 3. Dense Layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    \n",
    "    # 4. Output Layer\n",
    "    model.add(Dense(17038, activation='softmax'))\"\"\"\n",
    "    \n",
    "    # compire the model\n",
    "    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network\n",
    "model = simple_model((256, 256, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.001, patience=5, \n",
    "                                              verbose=0, mode='auto', baseline=None, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Raw_Generator(obs_id_train, y_train, BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#history = model.fit(generator, epochs=100, callbacks=[early_stop])\n",
    "history = model.fit(train_ds, epochs=100, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_simple_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
