{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in /opt/anaconda3/lib/python3.8/site-packages (0.1.22)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from opendatasets) (4.50.2)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.8/site-packages (from opendatasets) (7.1.2)\n",
      "Requirement already satisfied: kaggle in /opt/anaconda3/lib/python3.8/site-packages (from opendatasets) (1.5.12)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.8/site-packages (from kaggle->opendatasets) (2020.6.20)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from kaggle->opendatasets) (2.24.0)\n",
      "Requirement already satisfied: urllib3 in /opt/anaconda3/lib/python3.8/site-packages (from kaggle->opendatasets) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.8/site-packages (from kaggle->opendatasets) (2.8.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/anaconda3/lib/python3.8/site-packages (from kaggle->opendatasets) (1.15.0)\n",
      "Requirement already satisfied: python-slugify in /opt/anaconda3/lib/python3.8/site-packages (from kaggle->opendatasets) (6.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle->opendatasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/anaconda3/lib/python3.8/site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'GLC'...\n",
      "remote: Enumerating objects: 383, done.\u001b[K\n",
      "remote: Counting objects: 100% (228/228), done.\u001b[K\n",
      "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
      "remote: Total 383 (delta 119), reused 170 (delta 63), pack-reused 155\u001b[K\n",
      "Receiving objects: 100% (383/383), 10.57 MiB | 35.95 MiB/s, done.\n",
      "Resolving deltas: 100% (205/205), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf GLC\n",
    "!git clone https://github.com/maximiliense/GLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-yi7tcytj because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, GlobalAveragePooling2D, Conv2D, Dense, AveragePooling2D, \\\n",
    "BatchNormalization, Normalization, Dropout, Flatten, Lambda, Input, Activation\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import schedules, SGD\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import backend as K\n",
    "#import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import scale\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import copy\n",
    "import opendatasets as od\n",
    "\n",
    "%pylab inline --no-import-all\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"username\":\"nathaliemh\",\"key\":\"cba80d1f619e96b238e4a95aa3017836\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: nathaliemh\n",
      "Your Kaggle Key: ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2.00M/57.6G [00:00<52:06, 19.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading geolifeclef-2022-lifeclef-2022-fgvc9.zip to ./geolifeclef-2022-lifeclef-2022-fgvc9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57.6G/57.6G [29:45<00:00, 34.6MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive ./geolifeclef-2022-lifeclef-2022-fgvc9/geolifeclef-2022-lifeclef-2022-fgvc9.zip to ./geolifeclef-2022-lifeclef-2022-fgvc9\n"
     ]
    }
   ],
   "source": [
    "data = od.download(\"https://www.kaggle.com/competitions/geolifeclef-2022-lifeclef-2022-fgvc9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this path to adapt to where you downloaded the data\n",
    "DATA_PATH = Path(\"./geolifeclef-2022-lifeclef-2022-fgvc9/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "hours = 4\n",
    "#time.sleep(60*60*hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geolifeclef-2022-lifeclef-2022-fgvc9.zip  \u001b[0m\u001b[01;34mobservations\u001b[0m/  \u001b[01;34mpatches-us\u001b[0m/\r\n",
      "\u001b[01;34mmetadata\u001b[0m/                                 \u001b[01;34mpatches-fr\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls -L $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations_fr_test.csv   observations_us_test.csv\r\n",
      "observations_fr_train.csv  observations_us_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls $DATA_PATH/observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for training: 1627475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>species_id</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observation_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10561949</th>\n",
       "      <td>45.705116</td>\n",
       "      <td>1.424622</td>\n",
       "      <td>241</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10131188</th>\n",
       "      <td>45.146973</td>\n",
       "      <td>6.416794</td>\n",
       "      <td>101</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799362</th>\n",
       "      <td>46.783695</td>\n",
       "      <td>-2.072855</td>\n",
       "      <td>700</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10392536</th>\n",
       "      <td>48.604866</td>\n",
       "      <td>-2.825003</td>\n",
       "      <td>1456</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10335049</th>\n",
       "      <td>48.815567</td>\n",
       "      <td>-0.161431</td>\n",
       "      <td>157</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 latitude  longitude  species_id subset\n",
       "observation_id                                         \n",
       "10561949        45.705116   1.424622         241  train\n",
       "10131188        45.146973   6.416794         101  train\n",
       "10799362        46.783695  -2.072855         700  train\n",
       "10392536        48.604866  -2.825003        1456  train\n",
       "10335049        48.815567  -0.161431         157  train"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training Dataset ###\n",
    "# let's load the data from file\n",
    "df_obs_fr = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "df_obs = pd.concat((df_obs_fr, df_obs_us))\n",
    "\n",
    "print(\"Number of observations for training: {}\".format(len(df_obs)))\n",
    "\n",
    "# let's have a look at the data\n",
    "df_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for testing: 36421\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observation_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10782781</th>\n",
       "      <td>43.601788</td>\n",
       "      <td>6.940195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10364138</th>\n",
       "      <td>46.241711</td>\n",
       "      <td>0.683586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10692017</th>\n",
       "      <td>45.181095</td>\n",
       "      <td>1.533459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10222322</th>\n",
       "      <td>46.938450</td>\n",
       "      <td>5.298678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10241950</th>\n",
       "      <td>45.017433</td>\n",
       "      <td>0.960736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 latitude  longitude\n",
       "observation_id                      \n",
       "10782781        43.601788   6.940195\n",
       "10364138        46.241711   0.683586\n",
       "10692017        45.181095   1.533459\n",
       "10222322        46.938450   5.298678\n",
       "10241950        45.017433   0.960736"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test Dataset ###\n",
    "df_obs_fr_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "df_obs_test = pd.concat((df_obs_fr_test, df_obs_us_test))\n",
    "\n",
    "print(\"Number of observations for testing: {}\".format(len(df_obs_test)))\n",
    "\n",
    "df_obs_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m00\u001b[0m/  \u001b[01;34m07\u001b[0m/  \u001b[01;34m14\u001b[0m/  \u001b[01;34m21\u001b[0m/  \u001b[01;34m28\u001b[0m/  \u001b[01;34m35\u001b[0m/  \u001b[01;34m42\u001b[0m/  \u001b[01;34m49\u001b[0m/  \u001b[01;34m56\u001b[0m/  \u001b[01;34m63\u001b[0m/  \u001b[01;34m70\u001b[0m/  \u001b[01;34m77\u001b[0m/  \u001b[01;34m84\u001b[0m/  \u001b[01;34m91\u001b[0m/  \u001b[01;34m98\u001b[0m/\r\n",
      "\u001b[01;34m01\u001b[0m/  \u001b[01;34m08\u001b[0m/  \u001b[01;34m15\u001b[0m/  \u001b[01;34m22\u001b[0m/  \u001b[01;34m29\u001b[0m/  \u001b[01;34m36\u001b[0m/  \u001b[01;34m43\u001b[0m/  \u001b[01;34m50\u001b[0m/  \u001b[01;34m57\u001b[0m/  \u001b[01;34m64\u001b[0m/  \u001b[01;34m71\u001b[0m/  \u001b[01;34m78\u001b[0m/  \u001b[01;34m85\u001b[0m/  \u001b[01;34m92\u001b[0m/  \u001b[01;34m99\u001b[0m/\r\n",
      "\u001b[01;34m02\u001b[0m/  \u001b[01;34m09\u001b[0m/  \u001b[01;34m16\u001b[0m/  \u001b[01;34m23\u001b[0m/  \u001b[01;34m30\u001b[0m/  \u001b[01;34m37\u001b[0m/  \u001b[01;34m44\u001b[0m/  \u001b[01;34m51\u001b[0m/  \u001b[01;34m58\u001b[0m/  \u001b[01;34m65\u001b[0m/  \u001b[01;34m72\u001b[0m/  \u001b[01;34m79\u001b[0m/  \u001b[01;34m86\u001b[0m/  \u001b[01;34m93\u001b[0m/\r\n",
      "\u001b[01;34m03\u001b[0m/  \u001b[01;34m10\u001b[0m/  \u001b[01;34m17\u001b[0m/  \u001b[01;34m24\u001b[0m/  \u001b[01;34m31\u001b[0m/  \u001b[01;34m38\u001b[0m/  \u001b[01;34m45\u001b[0m/  \u001b[01;34m52\u001b[0m/  \u001b[01;34m59\u001b[0m/  \u001b[01;34m66\u001b[0m/  \u001b[01;34m73\u001b[0m/  \u001b[01;34m80\u001b[0m/  \u001b[01;34m87\u001b[0m/  \u001b[01;34m94\u001b[0m/\r\n",
      "\u001b[01;34m04\u001b[0m/  \u001b[01;34m11\u001b[0m/  \u001b[01;34m18\u001b[0m/  \u001b[01;34m25\u001b[0m/  \u001b[01;34m32\u001b[0m/  \u001b[01;34m39\u001b[0m/  \u001b[01;34m46\u001b[0m/  \u001b[01;34m53\u001b[0m/  \u001b[01;34m60\u001b[0m/  \u001b[01;34m67\u001b[0m/  \u001b[01;34m74\u001b[0m/  \u001b[01;34m81\u001b[0m/  \u001b[01;34m88\u001b[0m/  \u001b[01;34m95\u001b[0m/\r\n",
      "\u001b[01;34m05\u001b[0m/  \u001b[01;34m12\u001b[0m/  \u001b[01;34m19\u001b[0m/  \u001b[01;34m26\u001b[0m/  \u001b[01;34m33\u001b[0m/  \u001b[01;34m40\u001b[0m/  \u001b[01;34m47\u001b[0m/  \u001b[01;34m54\u001b[0m/  \u001b[01;34m61\u001b[0m/  \u001b[01;34m68\u001b[0m/  \u001b[01;34m75\u001b[0m/  \u001b[01;34m82\u001b[0m/  \u001b[01;34m89\u001b[0m/  \u001b[01;34m96\u001b[0m/\r\n",
      "\u001b[01;34m06\u001b[0m/  \u001b[01;34m13\u001b[0m/  \u001b[01;34m20\u001b[0m/  \u001b[01;34m27\u001b[0m/  \u001b[01;34m34\u001b[0m/  \u001b[01;34m41\u001b[0m/  \u001b[01;34m48\u001b[0m/  \u001b[01;34m55\u001b[0m/  \u001b[01;34m62\u001b[0m/  \u001b[01;34m69\u001b[0m/  \u001b[01;34m76\u001b[0m/  \u001b[01;34m83\u001b[0m/  \u001b[01;34m90\u001b[0m/  \u001b[01;34m97\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls $DATA_PATH/patches-fr/00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landcover_code</th>\n",
       "      <th>suggested_landcover_code</th>\n",
       "      <th>suggested_landcover_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Cultivated Crops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Cultivated Crops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>Broad-leaved Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Coniferous Forest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   landcover_code  suggested_landcover_code suggested_landcover_label\n",
       "0               0                         0              Missing Data\n",
       "1               1                        11          Cultivated Crops\n",
       "2               2                        11          Cultivated Crops\n",
       "3               3                         6       Broad-leaved Forest\n",
       "4               4                         7         Coniferous Forest"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_suggested_landcover_alignment = pd.read_csv(DATA_PATH / \"metadata\" / \"landcover_suggested_alignment.csv\", sep=\";\")\n",
    "df_suggested_landcover_alignment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data sources: 4\n",
      "Arrays shape: [(256, 256, 3), (256, 256), (256, 256), (256, 256)]\n",
      "Data types: [dtype('uint8'), dtype('uint8'), dtype('int16'), dtype('uint8')]\n"
     ]
    }
   ],
   "source": [
    "from GLC.data_loading.common import load_patch\n",
    "\n",
    "patch = load_patch(10171444, DATA_PATH)\n",
    "\n",
    "print(\"Number of data sources: {}\".format(len(patch)))\n",
    "print(\"Arrays shape: {}\".format([p.shape for p in patch]))\n",
    "print(\"Data types: {}\".format([p.dtype for p in patch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "landcover_mapping = df_suggested_landcover_alignment[\"suggested_landcover_code\"].values\n",
    "#patch = load_patch(10171444, DATA_PATH, landcover_mapping=landcover_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from GLC.plotting import visualize_observation_patch\\n\\n# Extracts land cover labels\\nlandcover_labels = df_suggested_landcover_alignment[[\"suggested_landcover_code\", \"suggested_landcover_label\"]].drop_duplicates().sort_values(\"suggested_landcover_code\")[\"suggested_landcover_label\"].values\\n\\nvisualize_observation_patch(patch, observation_data=df_obs.loc[10561900], landcover_labels=landcover_labels)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from GLC.plotting import visualize_observation_patch\n",
    "\n",
    "# Extracts land cover labels\n",
    "landcover_labels = df_suggested_landcover_alignment[[\"suggested_landcover_code\", \"suggested_landcover_label\"]].drop_duplicates().sort_values(\"suggested_landcover_code\")[\"suggested_landcover_label\"].values\n",
    "\n",
    "visualize_observation_patch(patch, observation_data=df_obs.loc[10561900], landcover_labels=landcover_labels)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'patch = load_patch(22068100, DATA_PATH, landcover_mapping=landcover_mapping)\\n\\nvisualize_observation_patch(patch, observation_data=df_obs.loc[22068100], landcover_labels=landcover_labels)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"patch = load_patch(22068100, DATA_PATH, landcover_mapping=landcover_mapping)\n",
    "\n",
    "visualize_observation_patch(patch, observation_data=df_obs.loc[22068100], landcover_labels=landcover_labels)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.shape(patch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Split Labels\n",
    "Retrieve the train/val split provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1587395 (97.5% of train observations)\n",
      "Validation set size: 40080 (2.5% of train observations)\n"
     ]
    }
   ],
   "source": [
    "obs_id_train = df_obs.index[df_obs[\"subset\"] == \"train\"].values\n",
    "obs_id_val = df_obs.index[df_obs[\"subset\"] == \"val\"].values\n",
    "\n",
    "y_train = df_obs.loc[obs_id_train][\"species_id\"].values\n",
    "y_val = df_obs.loc[obs_id_val][\"species_id\"].values\n",
    "\n",
    "n_val = len(obs_id_val)\n",
    "print(\"Training set size: {} ({:.1%} of train observations)\".format(len(y_train), len(y_train) / len(df_obs)))\n",
    "print(\"Validation set size: {} ({:.1%} of train observations)\".format(n_val, n_val / len(df_obs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1587395,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99.994%\n",
      "sample array shape:  (15874, 256, 256, 3)\n",
      "label array shape:  (1587395,)\n"
     ]
    }
   ],
   "source": [
    "# load training dataset samples\n",
    "# factor = 1 means load full training dataset\n",
    "# factor = 100 means load 1/100 of the full dataset\n",
    "factor = 100\n",
    "print_progress = time.time()\n",
    "\n",
    "X_train = list() #np.array((np.shape(y_train), 256, 256, 3))\n",
    "for obs_id in obs_id_train:\n",
    "    patch = load_patch(obs_id, DATA_PATH, landcover_mapping=landcover_mapping)\n",
    "    X_train.append(patch[0])\n",
    "    \n",
    "    percent_progress = len(X_train)/(len(y_train)/factor) * 100\n",
    "    \n",
    "    if time.time() - print_progress > 0.1:\n",
    "        sys.stdout.write('\\r')\n",
    "        # the exact output you're looking for:\n",
    "        sys.stdout.write(\"[%-20s] %.3f%%\" % ('='*int(percent_progress/5), percent_progress))\n",
    "        sys.stdout.flush()\n",
    "        print_progress = time.time()\n",
    "    \n",
    "    if len(X_train) >= (len(y_train)/factor):\n",
    "        break\n",
    "print()\n",
    "sys.stdout.write(\"[%-20s] %.3f%%\" % ('='*int(100/5), 100))\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "print(\"sample array shape: \", np.shape(X_train))\n",
    "print(\"label array shape: \", np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train[:len(X_train)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2254"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train[:len(X_train)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================] 100%\n",
      "sample array shape:  (40080, 256, 256, 3)\n",
      "label array shape:  (40080,)\n"
     ]
    }
   ],
   "source": [
    "# load validation dataset samples\n",
    "factor = 1\n",
    "\n",
    "X_val = list() #np.array((np.shape(y_train), 256, 256, 3))\n",
    "for obs_id in obs_id_val:\n",
    "    patch = load_patch(obs_id, DATA_PATH, landcover_mapping=landcover_mapping)\n",
    "    X_val.append(patch[0])\n",
    "    \n",
    "    percent_progress = len(X_val)/(len(y_val)/factor) * 100\n",
    "    sys.stdout.write('\\r')\n",
    "    # the exact output you're looking for:\n",
    "    sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(percent_progress/5), percent_progress))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if len(X_val) >= (len(y_val)/factor):\n",
    "        break\n",
    "\n",
    "print()\n",
    "    \n",
    "X_val = np.array(X_val)\n",
    "print(\"sample array shape: \", np.shape(X_val))\n",
    "print(\"label array shape: \", np.shape(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val[:len(X_val)]))\n",
    "val_ds = val_ds.batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Simple Neural Network\n",
    "Let's create a first neural network as a baseline to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a 10 layer ReLU model of width 2\n",
    "def simple_model(input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # 1. Preprocessing\n",
    "    # rescale inputs\n",
    "    model.add(tf.keras.layers.Rescaling(1./255))\n",
    "\n",
    "    # 2. Convolutional Layers\n",
    "    model.add(Conv2D(32, kernel_size=5, activation='relu', input_shape=input_shape, padding='same'))\n",
    "    #model.add(AveragePooling2D())\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=5, activation='relu', padding='same'))\n",
    "    #model.add(AveragePooling2D())\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=5, activation='relu', padding='same'))\n",
    "    \n",
    "    # from convolutional layers to dense layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    # 3. Dense Layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    \n",
    "    # 4. Output Layer\n",
    "    model.add(Dense(4908, activation='softmax'))\n",
    "    \n",
    "    # compire the model\n",
    "    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network\n",
    "model = simple_model((256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4907"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y_train[:len(X_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in y_train[:len(X_train)]:\n",
    "    if y > np.max(y_train[:len(X_train)]):\n",
    "        print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(y_train[:len(X_train)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.001, patience=5, \n",
    "                                              verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "249/249 [==============================] - 89s 187ms/step - loss: 7.8787 - accuracy: 0.0016\n",
      "Epoch 2/100\n",
      "249/249 [==============================] - 41s 165ms/step - loss: 7.2664 - accuracy: 0.0013\n",
      "Epoch 3/100\n",
      "249/249 [==============================] - 41s 165ms/step - loss: 7.1873 - accuracy: 0.0016\n",
      "Epoch 4/100\n",
      "249/249 [==============================] - 41s 165ms/step - loss: 7.0479 - accuracy: 0.0023\n",
      "Epoch 5/100\n",
      "249/249 [==============================] - 44s 175ms/step - loss: 6.8894 - accuracy: 0.0029\n",
      "Epoch 6/100\n",
      "249/249 [==============================] - 41s 165ms/step - loss: 6.7966 - accuracy: 0.0033\n",
      "Epoch 7/100\n",
      "249/249 [==============================] - 44s 175ms/step - loss: 6.6894 - accuracy: 0.0040\n",
      "Epoch 8/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 6.5653 - accuracy: 0.0063\n",
      "Epoch 9/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 6.4466 - accuracy: 0.0086\n",
      "Epoch 10/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 6.3497 - accuracy: 0.0120\n",
      "Epoch 11/100\n",
      "249/249 [==============================] - 43s 175ms/step - loss: 6.2796 - accuracy: 0.0146\n",
      "Epoch 12/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 6.0364 - accuracy: 0.0230\n",
      "Epoch 13/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 5.8158 - accuracy: 0.0341\n",
      "Epoch 14/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 5.6319 - accuracy: 0.0475\n",
      "Epoch 15/100\n",
      "249/249 [==============================] - 43s 175ms/step - loss: 5.5197 - accuracy: 0.0603\n",
      "Epoch 16/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 5.4047 - accuracy: 0.0737\n",
      "Epoch 17/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 5.2065 - accuracy: 0.0932\n",
      "Epoch 18/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 4.9728 - accuracy: 0.1202\n",
      "Epoch 19/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 4.7328 - accuracy: 0.1503\n",
      "Epoch 20/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 4.5588 - accuracy: 0.1727\n",
      "Epoch 21/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 4.3473 - accuracy: 0.2032\n",
      "Epoch 22/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 4.1883 - accuracy: 0.2259\n",
      "Epoch 23/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 4.0246 - accuracy: 0.2500\n",
      "Epoch 24/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 3.8929 - accuracy: 0.2764\n",
      "Epoch 25/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 3.7547 - accuracy: 0.2977\n",
      "Epoch 26/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 3.5949 - accuracy: 0.3230\n",
      "Epoch 27/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 3.3620 - accuracy: 0.3564\n",
      "Epoch 28/100\n",
      "249/249 [==============================] - 44s 175ms/step - loss: 3.1319 - accuracy: 0.3898\n",
      "Epoch 29/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 2.9701 - accuracy: 0.4122\n",
      "Epoch 30/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 2.8698 - accuracy: 0.4296\n",
      "Epoch 31/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 2.7381 - accuracy: 0.4559\n",
      "Epoch 32/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 2.6005 - accuracy: 0.4809\n",
      "Epoch 33/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 2.4807 - accuracy: 0.5035\n",
      "Epoch 34/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 2.3640 - accuracy: 0.5263\n",
      "Epoch 35/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 2.2715 - accuracy: 0.5452\n",
      "Epoch 36/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 2.1637 - accuracy: 0.5665\n",
      "Epoch 37/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 2.0374 - accuracy: 0.5847\n",
      "Epoch 38/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 1.8576 - accuracy: 0.6185\n",
      "Epoch 39/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 1.7192 - accuracy: 0.6475\n",
      "Epoch 40/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 1.5879 - accuracy: 0.6705\n",
      "Epoch 41/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 1.5018 - accuracy: 0.6910\n",
      "Epoch 42/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 1.4092 - accuracy: 0.7081\n",
      "Epoch 43/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 1.3603 - accuracy: 0.7182\n",
      "Epoch 44/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 1.2524 - accuracy: 0.7355\n",
      "Epoch 45/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 1.1994 - accuracy: 0.7470\n",
      "Epoch 46/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 1.1565 - accuracy: 0.7617\n",
      "Epoch 47/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 1.0636 - accuracy: 0.7755\n",
      "Epoch 48/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 1.0261 - accuracy: 0.7832\n",
      "Epoch 49/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.9783 - accuracy: 0.7947\n",
      "Epoch 50/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.9499 - accuracy: 0.8020\n",
      "Epoch 51/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.9005 - accuracy: 0.8109\n",
      "Epoch 52/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.8365 - accuracy: 0.8236\n",
      "Epoch 53/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.8014 - accuracy: 0.8305\n",
      "Epoch 54/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.7662 - accuracy: 0.8389\n",
      "Epoch 55/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.7376 - accuracy: 0.8493\n",
      "Epoch 56/100\n",
      "249/249 [==============================] - 44s 175ms/step - loss: 0.6916 - accuracy: 0.8555\n",
      "Epoch 57/100\n",
      "249/249 [==============================] - 43s 175ms/step - loss: 0.6665 - accuracy: 0.8624\n",
      "Epoch 58/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.6300 - accuracy: 0.8694\n",
      "Epoch 59/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.6100 - accuracy: 0.8746\n",
      "Epoch 60/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.5706 - accuracy: 0.8804\n",
      "Epoch 61/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.5660 - accuracy: 0.8826\n",
      "Epoch 62/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.5332 - accuracy: 0.8891\n",
      "Epoch 63/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.5039 - accuracy: 0.8952\n",
      "Epoch 64/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.4784 - accuracy: 0.8993\n",
      "Epoch 65/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.4666 - accuracy: 0.9046\n",
      "Epoch 66/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.4408 - accuracy: 0.9105\n",
      "Epoch 67/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.4094 - accuracy: 0.9162\n",
      "Epoch 68/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.3871 - accuracy: 0.9199\n",
      "Epoch 69/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.3797 - accuracy: 0.9229\n",
      "Epoch 70/100\n",
      "249/249 [==============================] - 43s 175ms/step - loss: 0.3643 - accuracy: 0.9267\n",
      "Epoch 71/100\n",
      "249/249 [==============================] - 43s 175ms/step - loss: 0.3641 - accuracy: 0.9280\n",
      "Epoch 72/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.3556 - accuracy: 0.9308\n",
      "Epoch 73/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.3473 - accuracy: 0.9323\n",
      "Epoch 74/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.3271 - accuracy: 0.9356\n",
      "Epoch 75/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.3035 - accuracy: 0.9403\n",
      "Epoch 76/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.2924 - accuracy: 0.9430\n",
      "Epoch 77/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.2856 - accuracy: 0.9453\n",
      "Epoch 78/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.2632 - accuracy: 0.9483\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 43s 174ms/step - loss: 0.2411 - accuracy: 0.9497\n",
      "Epoch 80/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.2452 - accuracy: 0.9514\n",
      "Epoch 81/100\n",
      "249/249 [==============================] - 43s 174ms/step - loss: 0.2309 - accuracy: 0.9538\n",
      "Epoch 82/100\n",
      "249/249 [==============================] - 41s 166ms/step - loss: 0.2203 - accuracy: 0.9542\n",
      "Epoch 83/100\n",
      "249/249 [==============================] - 43s 175ms/step - loss: 0.2275 - accuracy: 0.9558\n",
      "Epoch 84/100\n",
      "249/249 [==============================] - 41s 165ms/step - loss: 0.2159 - accuracy: 0.9560\n",
      "Epoch 85/100\n",
      " 78/249 [========>.....................] - ETA: 28s - loss: 0.1934 - accuracy: 0.9621"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, #validation_data=val_ds, #X_train, y_train[:len(X_train)], #validation_data=(X_val, y_val), \n",
    "                    epochs=100, \n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_simple_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
