{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U tensorflow-addons\n",
    "# !pip install huggingface-hub\n",
    "# !pip install transformers\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Add, GlobalAveragePooling2D, Conv2D, Dense, AveragePooling2D, BatchNormalization, Dropout, Flatten, Lambda, Input, Activation\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import schedules, SGD\n",
    "from tensorflow.keras.callbacks import Callback, TensorBoard as TensorboardCallback, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from huggingface_hub import notebook_login, HfFolder, HfApi\n",
    "\n",
    "from transformers import TFViTForImageClassification, create_optimizer, ViTFeatureExtractor\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import scale\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import copy\n",
    "import opendatasets as od\n",
    "# import cartopy\n",
    "\n",
    "%pylab inline --no-import-all\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this path to adapt to where you downloaded the data\n",
    "DATA_PATH = Path(\"./geolifeclef-2022-lifeclef-2022-fgvc9/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "hours = 4\n",
    "#time.sleep(60*60*hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for training: 1627475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>species_id</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observation_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10561949</th>\n",
       "      <td>45.705116</td>\n",
       "      <td>1.424622</td>\n",
       "      <td>241</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10131188</th>\n",
       "      <td>45.146973</td>\n",
       "      <td>6.416794</td>\n",
       "      <td>101</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799362</th>\n",
       "      <td>46.783695</td>\n",
       "      <td>-2.072855</td>\n",
       "      <td>700</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10392536</th>\n",
       "      <td>48.604866</td>\n",
       "      <td>-2.825003</td>\n",
       "      <td>1456</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10335049</th>\n",
       "      <td>48.815567</td>\n",
       "      <td>-0.161431</td>\n",
       "      <td>157</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 latitude  longitude  species_id subset\n",
       "observation_id                                         \n",
       "10561949        45.705116   1.424622         241  train\n",
       "10131188        45.146973   6.416794         101  train\n",
       "10799362        46.783695  -2.072855         700  train\n",
       "10392536        48.604866  -2.825003        1456  train\n",
       "10335049        48.815567  -0.161431         157  train"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training Dataset ###\n",
    "# let's load the data from file\n",
    "df_obs_fr = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "df_obs = pd.concat((df_obs_fr, df_obs_us))\n",
    "\n",
    "print(\"Number of observations for training: {}\".format(len(df_obs)))\n",
    "\n",
    "# let's have a look at the data\n",
    "df_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for testing: 36421\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observation_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10782781</th>\n",
       "      <td>43.601788</td>\n",
       "      <td>6.940195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10364138</th>\n",
       "      <td>46.241711</td>\n",
       "      <td>0.683586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10692017</th>\n",
       "      <td>45.181095</td>\n",
       "      <td>1.533459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10222322</th>\n",
       "      <td>46.938450</td>\n",
       "      <td>5.298678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10241950</th>\n",
       "      <td>45.017433</td>\n",
       "      <td>0.960736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 latitude  longitude\n",
       "observation_id                      \n",
       "10782781        43.601788   6.940195\n",
       "10364138        46.241711   0.683586\n",
       "10692017        45.181095   1.533459\n",
       "10222322        46.938450   5.298678\n",
       "10241950        45.017433   0.960736"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test Dataset ###\n",
    "df_obs_fr_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "df_obs_test = pd.concat((df_obs_fr_test, df_obs_us_test))\n",
    "\n",
    "print(\"Number of observations for testing: {}\".format(len(df_obs_test)))\n",
    "\n",
    "df_obs_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landcover_code</th>\n",
       "      <th>suggested_landcover_code</th>\n",
       "      <th>suggested_landcover_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Cultivated Crops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Cultivated Crops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>Broad-leaved Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Coniferous Forest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   landcover_code  suggested_landcover_code suggested_landcover_label\n",
       "0               0                         0              Missing Data\n",
       "1               1                        11          Cultivated Crops\n",
       "2               2                        11          Cultivated Crops\n",
       "3               3                         6       Broad-leaved Forest\n",
       "4               4                         7         Coniferous Forest"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_suggested_landcover_alignment = pd.read_csv(DATA_PATH / \"metadata\" / \"landcover_suggested_alignment.csv\", sep=\";\")\n",
    "df_suggested_landcover_alignment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data sources: 4\n",
      "Arrays shape: [(256, 256, 3), (256, 256), (256, 256), (256, 256)]\n",
      "Data types: [dtype('uint8'), dtype('uint8'), dtype('int16'), dtype('uint8')]\n"
     ]
    }
   ],
   "source": [
    "from GLC.data_loading.common import load_patch\n",
    "\n",
    "patch = load_patch(10171444, DATA_PATH)\n",
    "\n",
    "print(\"Number of data sources: {}\".format(len(patch)))\n",
    "print(\"Arrays shape: {}\".format([p.shape for p in patch]))\n",
    "print(\"Data types: {}\".format([p.dtype for p in patch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "landcover_mapping = df_suggested_landcover_alignment[\"suggested_landcover_code\"].values\n",
    "#patch = load_patch(10171444, DATA_PATH, landcover_mapping=landcover_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from GLC.plotting import visualize_observation_patch\n",
    "\n",
    "# # Extracts land cover labels\n",
    "# landcover_labels = df_suggested_landcover_alignment[[\"suggested_landcover_code\", \"suggested_landcover_label\"]].drop_duplicates().sort_values(\"suggested_landcover_code\")[\"suggested_landcover_label\"].values\n",
    "\n",
    "# visualize_observation_patch(patch, observation_data=df_obs.loc[10561900], landcover_labels=landcover_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'patch = load_patch(22068100, DATA_PATH, landcover_mapping=landcover_mapping)\\n\\nvisualize_observation_patch(patch, observation_data=df_obs.loc[22068100], landcover_labels=landcover_labels)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"patch = load_patch(22068100, DATA_PATH, landcover_mapping=landcover_mapping)\n",
    "\n",
    "visualize_observation_patch(patch, observation_data=df_obs.loc[22068100], landcover_labels=landcover_labels)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Split Labels\n",
    "Retrieve the train/val split provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1587395 (97.5% of train observations)\n",
      "Validation set size: 40080 (2.5% of train observations)\n"
     ]
    }
   ],
   "source": [
    "obs_id_train = df_obs.index[df_obs[\"subset\"] == \"train\"].values\n",
    "obs_id_val = df_obs.index[df_obs[\"subset\"] == \"val\"].values\n",
    "\n",
    "y_train = df_obs.loc[obs_id_train][\"species_id\"].values\n",
    "y_val = df_obs.loc[obs_id_val][\"species_id\"].values\n",
    "\n",
    "n_val = len(obs_id_val)\n",
    "print(\"Training set size: {} ({:.1%} of train observations)\".format(len(y_train), len(y_train) / len(df_obs)))\n",
    "print(\"Validation set size: {} ({:.1%} of train observations)\".format(n_val, n_val / len(df_obs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load training dataset samples\n",
    "# factor = 1 means load full training dataset\n",
    "# factor = 100 means load 1/100 of the full dataset\n",
    "\n",
    "def load_train_data(factor):\n",
    "    X_train = list() #np.array((np.shape(y_train), 256, 256, 3))\n",
    "    for obs_id in obs_id_train:\n",
    "        patch = load_patch(obs_id, DATA_PATH, landcover_mapping=landcover_mapping)\n",
    "        X_train.append(patch[0])\n",
    "\n",
    "        percent_progress = len(X_train)/(len(obs_id_train)/factor) * 100\n",
    "        sys.stdout.write('\\r')\n",
    "        # the exact output you're looking for:\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(percent_progress/5), percent_progress))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if len(X_train) >= (len(obs_id_train)/factor):\n",
    "            break\n",
    "    print()\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    return X_train\n",
    "\n",
    "def load_val_data(factor):\n",
    "    X_val = list() #np.array((np.shape(y_train), 256, 256, 3))\n",
    "    for obs_id in obs_id_val:\n",
    "        patch = load_patch(obs_id, DATA_PATH, landcover_mapping=landcover_mapping)\n",
    "        X_val.append(patch[0])\n",
    "\n",
    "        percent_progress = len(X_val)/(len(y_val)/factor) * 100\n",
    "        sys.stdout.write('\\r')\n",
    "        # the exact output you're looking for:\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(percent_progress/5), percent_progress))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if len(X_val) >= (len(y_val)/factor):\n",
    "            break\n",
    "\n",
    "    print()\n",
    "\n",
    "    X_val = np.array(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 1000\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((load_train_data(factor), y_train[:obs_id_train//factor]))\n",
    "# train_ds = train_ds.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(df_obs['species_id']))\n",
    "input_shape = (256, 256, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val[:len(X_val)]))\n",
    "# val_ds = val_ds.batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer - ViT - from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.001\n",
    "# weight_decay = 0.0001\n",
    "# batch_size = 256\n",
    "# num_epochs = 50\n",
    "# image_size = 72  # We'll resize input images to this size\n",
    "# patch_size = 6  # Size of the patches to be extract from the input images\n",
    "# num_patches = (image_size // patch_size) ** 2\n",
    "# projection_dim = 64\n",
    "# num_heads = 4\n",
    "# transformer_units = [\n",
    "#     projection_dim * 2,\n",
    "#     projection_dim,\n",
    "# ]  # Size of the transformer layers\n",
    "# transformer_layers = 8\n",
    "# mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data augmentation\n",
    "# data_augmentation = keras.Sequential(\n",
    "#     [\n",
    "#         layers.Normalization(),\n",
    "#         layers.Resizing(image_size, image_size),\n",
    "#         layers.RandomFlip(\"horizontal\"),\n",
    "#         layers.RandomRotation(factor=0.02),\n",
    "#         layers.RandomZoom(\n",
    "#             height_factor=0.2, width_factor=0.2\n",
    "#         ),\n",
    "#     ],\n",
    "#     name=\"data_augmentation\",\n",
    "# )\n",
    "# # Compute the mean and the variance of the training data for normalization.\n",
    "# data_augmentation.layers[0].adapt(X_train)\n",
    "\n",
    "# # multi-layer perceptron\n",
    "# def mlp(x, hidden_units, dropout_rate):\n",
    "#     for units in hidden_units:\n",
    "#         x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "#         x = layers.Dropout(dropout_rate)(x)\n",
    "#     return x\n",
    "\n",
    "# # Patch creation\n",
    "# class Patches(layers.Layer):\n",
    "#     def __init__(self, patch_size):\n",
    "#         super(Patches, self).__init__()\n",
    "#         self.patch_size = patch_size\n",
    "\n",
    "#     def call(self, images):\n",
    "#         batch_size = tf.shape(images)[0]\n",
    "#         patches = tf.image.extract_patches(\n",
    "#             images=images,\n",
    "#             sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "#             strides=[1, self.patch_size, self.patch_size, 1],\n",
    "#             rates=[1, 1, 1, 1],\n",
    "#             padding=\"VALID\",\n",
    "#         )\n",
    "#         patch_dims = patches.shape[-1]\n",
    "#         patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "#         return patches\n",
    "    \n",
    "# class PatchEncoder(layers.Layer):\n",
    "#     def __init__(self, num_patches, projection_dim):\n",
    "#         super(PatchEncoder, self).__init__()\n",
    "#         self.num_patches = num_patches\n",
    "#         self.projection = layers.Dense(units=projection_dim)\n",
    "#         self.position_embedding = layers.Embedding(\n",
    "#             input_dim=num_patches, output_dim=projection_dim\n",
    "#         )\n",
    "\n",
    "#     def call(self, patch):\n",
    "#         positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "#         encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "#         return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize patches\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# image = X_train[np.random.choice(range(X_train.shape[0]))]\n",
    "# plt.imshow(image.astype(\"uint8\"))\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# resized_image = tf.image.resize(\n",
    "#     tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    "# )\n",
    "# patches = Patches(patch_size)(resized_image)\n",
    "# print(f\"Image size: {image_size} X {image_size}\")\n",
    "# print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "# print(f\"Patches per image: {patches.shape[1]}\")\n",
    "# print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "# n = int(np.sqrt(patches.shape[1]))\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# for i, patch in enumerate(patches[0]):\n",
    "#     ax = plt.subplot(n, n, i + 1)\n",
    "#     patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "#     plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_vit_classifier():\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "#     # Augment data.\n",
    "#     augmented = data_augmentation(inputs)\n",
    "#     # Create patches.\n",
    "#     patches = Patches(patch_size)(augmented)\n",
    "#     # Encode patches.\n",
    "#     encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "#     # Create multiple layers of the Transformer block.\n",
    "#     for _ in range(transformer_layers):\n",
    "#         # Layer normalization 1.\n",
    "#         x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "#         # Create a multi-head attention layer.\n",
    "#         attention_output = layers.MultiHeadAttention(\n",
    "#             num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "#         )(x1, x1)\n",
    "#         # Skip connection 1.\n",
    "#         x2 = layers.Add()([attention_output, encoded_patches])\n",
    "#         # Layer normalization 2.\n",
    "#         x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "#         # MLP.\n",
    "#         x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "#         # Skip connection 2.\n",
    "#         encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "#     # Create a [batch_size, projection_dim] tensor.\n",
    "#     representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "#     representation = layers.Flatten()(representation)\n",
    "#     representation = layers.Dropout(0.5)(representation)\n",
    "#     # Add MLP.\n",
    "#     features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    \n",
    "#     # Classify outputs.\n",
    "#     logits = layers.Dense(num_classes)(features)\n",
    "#     # Create the Keras model.\n",
    "#     model = keras.Model(inputs=inputs, outputs=logits)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_experiment(model):\n",
    "#     optimizer = tfa.optimizers.AdamW(\n",
    "#         learning_rate=learning_rate, weight_decay=weight_decay\n",
    "#     )\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=optimizer,\n",
    "#         loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#         metrics=[\n",
    "#             keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "#             keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "#         ],\n",
    "#     )\n",
    "\n",
    "#     checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "#     checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "#         checkpoint_filepath,\n",
    "#         monitor=\"val_accuracy\",\n",
    "#         save_best_only=True,\n",
    "#         save_weights_only=True,\n",
    "#     )\n",
    "\n",
    "#     history = model.fit(\n",
    "#         X_train, y_train[:len(X_train)],\n",
    "#         batch_size=batch_size,\n",
    "#         epochs=num_epochs,\n",
    "#         validation_data=val_ds,\n",
    "#         callbacks=[checkpoint_callback],\n",
    "#     )\n",
    "\n",
    "#     model.load_weights(checkpoint_filepath)\n",
    "# #     _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "# #     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "# #     print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "#     return history\n",
    "\n",
    "\n",
    "# vit_classifier = create_vit_classifier()\n",
    "# history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vit_classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer - ViT - pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b915769543548f3a61726f68bff4eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/vit-base-patch16-224-in21k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "\n",
    "def create_dataset(y, factor=1000):\n",
    "    \"\"\"creates `Dataset` from image folder structure\"\"\"    \n",
    "    \n",
    "    # defines `datasets` features`\n",
    "    labels = list(set(y))\n",
    "    labels = list(np.char.mod('%d', labels))\n",
    "    features=datasets.Features({\n",
    "                      \"pixel_values\": datasets.Array3D((3, 224, 224), dtype='int16'),\n",
    "                      \"label\": datasets.features.ClassLabel(names = labels),\n",
    "                  })\n",
    "    # create dataset\n",
    "    \n",
    "    X = load_train_data(factor)\n",
    "    \n",
    "    processed_X = zoom(X, (1, 224/256, 224/256, 1))\n",
    "    processed_X = np.swapaxes(processed_X, 1, -1) # Can you just do this?\n",
    "    \n",
    "    print(processed_X.shape)\n",
    "    \n",
    "    y = np.char.mod('%d', y)\n",
    "    ds = datasets.Dataset.from_dict({\"pixel_values\": processed_X, \"label\": y[:len(X)]}, features=features)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================] 100%\n",
      "(318, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "train_ds = create_dataset(y_train, factor=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['pixel_values', 'label'],\n",
      "    num_rows: 318\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_class_labels = train_ds.features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 20:33:30.116789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:30.117644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:30.254212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:30.254948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:30.255575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:30.256199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:30.257601: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-25 20:33:30.736635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:30.737364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:30.737997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:30.738643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:30.739257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:30.739871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:32.505379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:32.506152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:32.506807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:32.507437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:32.508057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:32.509864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13823 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2022-04-25 20:33:32.512557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:33:32.513208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13823 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_id)\n",
    "\n",
    "# learn more about data augmentation here: https://www.tensorflow.org/tutorials/images/data_augmentation\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.Rescaling(1./255),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# use keras image data augementation processing\n",
    "def augmentation(examples):\n",
    "    # print(examples[\"img\"])\n",
    "    examples[\"pixel_values\"] = [data_augmentation(image) for image in examples[\"pixel_values\"]]\n",
    "    return examples\n",
    "\n",
    "# basic processing (only resizing)\n",
    "def process(examples):\n",
    "    examples.update(feature_extractor(examples['pixel_values'], ))\n",
    "    return examples\n",
    " \n",
    "# we are also renaming our label col to labels to use `.to_tf_dataset` later\n",
    "train_ds = train_ds.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmenting dataset takes a lot of time\n",
    "# processed_dataset = train_ds.map(process, batched=True)\n",
    "# processed_dataset\n",
    "\n",
    "# processed_dataset = eurosat_ds.map(augmentation, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test size will be 15% of train dataset\n",
    "test_size=.15\n",
    "\n",
    "processed_dataset = train_ds.shuffle().train_test_split(test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 50\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "learning_rate = 3e-5\n",
    "weight_decay_rate=0.01\n",
    "num_warmup_steps=0\n",
    "output_dir=model_id.split(\"/\")[1]\n",
    "hub_token = \"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\" # or your token directly \"hf_xxx\"\n",
    "hub_model_id = f'{model_id.split(\"/\")[1]}-species-prediction'\n",
    "fp16=True\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "# Comment this line out if you're using a GPU that will not benefit from this\n",
    "if fp16:\n",
    "    keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array3D(shape=(3, 224, 224), dtype='int16', id=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset['train'].features['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "# Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "# converting our train dataset to tf.data.Dataset\n",
    "tf_train_dataset = processed_dataset[\"train\"].to_tf_dataset(\n",
    "   columns=['pixel_values'],\n",
    "   label_cols=[\"labels\"],\n",
    "   shuffle=True,\n",
    "   batch_size=train_batch_size,\n",
    "   collate_fn=data_collator)\n",
    "\n",
    "# converting our test dataset to tf.data.Dataset\n",
    "tf_eval_dataset = processed_dataset[\"test\"].to_tf_dataset(\n",
    "   columns=['pixel_values'],\n",
    "   label_cols=[\"labels\"],\n",
    "   shuffle=True,\n",
    "   batch_size=eval_batch_size,\n",
    "   collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/bias:0', 'vit/pooler/dense/kernel:0']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer wight weigh decay\n",
    "num_train_steps = len(train_ds) * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    ")\n",
    "\n",
    "# load pre-trained ViT model\n",
    "base_model = TFViTForImageClassification.from_pretrained(model_id)\n",
    "\n",
    "# Inputs\n",
    "pixel_values = layers.Input(shape=(3, 224, 224), name='pixel_values', dtype='float32')\n",
    "\n",
    "# Pre-trained ViT model\n",
    "vit = base_model.vit(pixel_values)[0]\n",
    "\n",
    "# Add classification head\n",
    "classifier = tf.keras.layers.Dense(num_classes, name='outputs')(vit[:, 0, :])\n",
    "\n",
    "model = tf.keras.Model(inputs=pixel_values, outputs=classifier)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, \n",
    "                                 weight_decay=weight_decay_rate)\n",
    "# Compile model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\n",
    "                  tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "                  tf.keras.metrics.SparseTopKCategoricalAccuracy(10, name=\"top-10-accuracy\")\n",
    "              ]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=[]\n",
    "\n",
    "callbacks.append(TensorboardCallback(log_dir=os.path.join(output_dir,\"logs\")))\n",
    "# callbacks.append(EarlyStopping(monitor=\"val_accuracy\",patience=1))\n",
    "# if hub_token:\n",
    "#     callbacks.append(PushToHubCallback(output_dir=output_dir,\n",
    "#                                        hub_model_id=hub_model_id,\n",
    "#                                        hub_token=hub_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 24s 1s/step - loss: 9.7422 - accuracy: 0.0000e+00 - top-10-accuracy: 0.0000e+00 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 7s 817ms/step - loss: 9.7080 - accuracy: 0.0117 - top-10-accuracy: 0.0312 - val_loss: 9.7266 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 7s 831ms/step - loss: 9.6670 - accuracy: 0.0117 - top-10-accuracy: 0.0703 - val_loss: 9.7266 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 10s 1s/step - loss: 9.6367 - accuracy: 0.0117 - top-10-accuracy: 0.0820 - val_loss: 9.7109 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 7s 803ms/step - loss: 9.6162 - accuracy: 0.0117 - top-10-accuracy: 0.1055 - val_loss: 9.7109 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 7s 794ms/step - loss: 9.6025 - accuracy: 0.0117 - top-10-accuracy: 0.1055 - val_loss: 9.7031 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 7s 824ms/step - loss: 9.5947 - accuracy: 0.0117 - top-10-accuracy: 0.1055 - val_loss: 9.7031 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 7s 798ms/step - loss: 9.5889 - accuracy: 0.0117 - top-10-accuracy: 0.1016 - val_loss: 9.6953 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 7s 806ms/step - loss: 9.5908 - accuracy: 0.0117 - top-10-accuracy: 0.0977 - val_loss: 9.7109 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 7s 808ms/step - loss: 9.5928 - accuracy: 0.0117 - top-10-accuracy: 0.0898 - val_loss: 9.6953 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 7s 793ms/step - loss: 9.5957 - accuracy: 0.0117 - top-10-accuracy: 0.0938 - val_loss: 9.6953 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 7s 789ms/step - loss: 9.6055 - accuracy: 0.0117 - top-10-accuracy: 0.0898 - val_loss: 9.6953 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 7s 797ms/step - loss: 9.6152 - accuracy: 0.0117 - top-10-accuracy: 0.0820 - val_loss: 9.7031 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 7s 810ms/step - loss: 9.6221 - accuracy: 0.0117 - top-10-accuracy: 0.0977 - val_loss: 9.7109 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 7s 784ms/step - loss: 9.6289 - accuracy: 0.0117 - top-10-accuracy: 0.0977 - val_loss: 9.7188 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 7s 798ms/step - loss: 9.6367 - accuracy: 0.0117 - top-10-accuracy: 0.0938 - val_loss: 9.7031 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 7s 814ms/step - loss: 9.6416 - accuracy: 0.0117 - top-10-accuracy: 0.0898 - val_loss: 9.7109 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 7s 790ms/step - loss: 9.6504 - accuracy: 0.0117 - top-10-accuracy: 0.0859 - val_loss: 9.7188 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 7s 827ms/step - loss: 9.6562 - accuracy: 0.0117 - top-10-accuracy: 0.0938 - val_loss: 9.7109 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 7s 850ms/step - loss: 9.6592 - accuracy: 0.0117 - top-10-accuracy: 0.0898 - val_loss: 9.7188 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 7s 819ms/step - loss: 9.6641 - accuracy: 0.0117 - top-10-accuracy: 0.0938 - val_loss: 9.7188 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 7s 793ms/step - loss: 9.6680 - accuracy: 0.0117 - top-10-accuracy: 0.0938 - val_loss: 9.7109 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 7s 802ms/step - loss: 9.6719 - accuracy: 0.0117 - top-10-accuracy: 0.0898 - val_loss: 9.7188 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 7s 804ms/step - loss: 9.6758 - accuracy: 0.0117 - top-10-accuracy: 0.0898 - val_loss: 9.7188 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 7s 799ms/step - loss: 9.6787 - accuracy: 0.0117 - top-10-accuracy: 0.0859 - val_loss: 9.7188 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 7s 802ms/step - loss: 9.6836 - accuracy: 0.0117 - top-10-accuracy: 0.0938 - val_loss: 9.7266 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 7s 811ms/step - loss: 9.6875 - accuracy: 0.0117 - top-10-accuracy: 0.1016 - val_loss: 9.7266 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 7s 813ms/step - loss: 9.6895 - accuracy: 0.0117 - top-10-accuracy: 0.0977 - val_loss: 9.7266 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 7s 780ms/step - loss: 9.6953 - accuracy: 0.0117 - top-10-accuracy: 0.1016 - val_loss: 9.7266 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 7s 818ms/step - loss: 9.6973 - accuracy: 0.0117 - top-10-accuracy: 0.0938 - val_loss: 9.7266 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 7s 832ms/step - loss: 9.7031 - accuracy: 0.0078 - top-10-accuracy: 0.0898 - val_loss: 9.7266 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 7s 794ms/step - loss: 9.7031 - accuracy: 0.0156 - top-10-accuracy: 0.1055 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 7s 801ms/step - loss: 9.7041 - accuracy: 0.0156 - top-10-accuracy: 0.1016 - val_loss: 9.7266 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 7s 801ms/step - loss: 9.7109 - accuracy: 0.0156 - top-10-accuracy: 0.0898 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 7s 801ms/step - loss: 9.7109 - accuracy: 0.0156 - top-10-accuracy: 0.1016 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 7s 785ms/step - loss: 9.7109 - accuracy: 0.0156 - top-10-accuracy: 0.0977 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 7s 794ms/step - loss: 9.7119 - accuracy: 0.0117 - top-10-accuracy: 0.0977 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 7s 820ms/step - loss: 9.7188 - accuracy: 0.0156 - top-10-accuracy: 0.1055 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 7s 809ms/step - loss: 9.7188 - accuracy: 0.0156 - top-10-accuracy: 0.1055 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 7s 786ms/step - loss: 9.7188 - accuracy: 0.0156 - top-10-accuracy: 0.1016 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 7s 807ms/step - loss: 9.7188 - accuracy: 0.0156 - top-10-accuracy: 0.1016 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 7s 783ms/step - loss: 9.7188 - accuracy: 0.0156 - top-10-accuracy: 0.1016 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 7s 786ms/step - loss: 9.7197 - accuracy: 0.0117 - top-10-accuracy: 0.1016 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 7s 815ms/step - loss: 9.7266 - accuracy: 0.0156 - top-10-accuracy: 0.1016 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 7s 803ms/step - loss: 9.7266 - accuracy: 0.0156 - top-10-accuracy: 0.1094 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 7s 783ms/step - loss: 9.7266 - accuracy: 0.0156 - top-10-accuracy: 0.1016 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 7s 849ms/step - loss: 9.7266 - accuracy: 0.0117 - top-10-accuracy: 0.0977 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 7s 791ms/step - loss: 9.7266 - accuracy: 0.0156 - top-10-accuracy: 0.0898 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 7s 783ms/step - loss: 9.7266 - accuracy: 0.0156 - top-10-accuracy: 0.1016 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 7s 776ms/step - loss: 9.7266 - accuracy: 0.0156 - top-10-accuracy: 0.1055 - val_loss: 9.7344 - val_accuracy: 0.0000e+00 - val_top-10-accuracy: 0.0312\n"
     ]
    }
   ],
   "source": [
    "train_results = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_eval_dataset,\n",
    "    callbacks=callbacks,\n",
    "    epochs=num_train_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/jannikjw/vit-base-patch16-224-in21k-species-prediction/blob/main/preprocessor_config.json'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = HfApi()\n",
    "\n",
    "user = api.whoami(hub_token)\n",
    "\n",
    "feature_extractor.save_pretrained(output_dir)\n",
    "\n",
    "api.upload_file(\n",
    "    token=hub_token,\n",
    "    repo_id=f\"{user['name']}/{hub_model_id}\",\n",
    "    path_or_fileobj=os.path.join(output_dir,\"preprocessor_config.json\"),\n",
    "    path_in_repo=\"preprocessor_config.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Simple Neural Network\n",
    "Let's create a first neural network as a baseline to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a 10 layer ReLU model of width 2\n",
    "def simple_model(input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # 1. Preprocessing\n",
    "    # rescale inputs\n",
    "    model.add(tf.keras.layers.Rescaling(1./255))\n",
    "\n",
    "    # 2. Convolutional Layers\n",
    "    model.add(Conv2D(32, kernel_size=5, activation='relu', input_shape=input_shape, padding='same'))\n",
    "    #model.add(AveragePooling2D())\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=5, activation='relu', padding='same'))\n",
    "    #model.add(AveragePooling2D())\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=5, activation='relu', padding='same'))\n",
    "    \n",
    "    # from convolutional layers to dense layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    # 3. Dense Layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    \n",
    "    # 4. Output Layer\n",
    "    model.add(Dense(4911, activation='softmax'))\n",
    "    \n",
    "    # compire the model\n",
    "    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network\n",
    "model = simple_model((256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(y_train[:len(X_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(y_train[:len(X_train)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.001, patience=5, \n",
    "                                              verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, #X_train, y_train[:len(X_train)], #validation_data=(X_val, y_val), \n",
    "                    epochs=100, \n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_simple_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
