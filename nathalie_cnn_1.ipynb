{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install opendatasets\n",
    "#!pip install cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'GLC'...\n",
      "remote: Enumerating objects: 383, done.\u001b[K\n",
      "remote: Counting objects: 100% (228/228), done.\u001b[K\n",
      "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
      "remote: Total 383 (delta 119), reused 170 (delta 63), pack-reused 155\u001b[K\n",
      "Receiving objects: 100% (383/383), 10.57 MiB | 35.14 MiB/s, done.\n",
      "Resolving deltas: 100% (205/205), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf GLC\n",
    "!git clone https://github.com/maximiliense/GLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, GlobalAveragePooling2D, Conv2D, Dense, AveragePooling2D, \\\n",
    "BatchNormalization, Normalization, Dropout, Flatten, Lambda, Input, Activation, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import schedules, SGD\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import backend as K\n",
    "#import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import scale\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import copy\n",
    "import threading\n",
    "import opendatasets as od\n",
    "\n",
    "%pylab inline --no-import-all\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from GLC.data_loading.common import load_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only uncomment if you don't have the dataset stored on disk yet\n",
    "# -> have your kaggle user credentials ready\n",
    "# data = od.download(\"https://www.kaggle.com/competitions/geolifeclef-2022-lifeclef-2022-fgvc9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path to competition dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this path to adapt to where you downloaded the data\n",
    "DATA_PATH = Path(\"./geolifeclef-2022-lifeclef-2022-fgvc9/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following two commands to verify that the data path is set correctly. They should print folder and file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmetadata\u001b[0m/      \u001b[01;34mpatches-fr\u001b[0m/  \u001b[01;34mpatches_sample\u001b[0m/  \u001b[01;34mrasters\u001b[0m/\r\n",
      "\u001b[01;34mobservations\u001b[0m/  \u001b[01;34mpatches-us\u001b[0m/  \u001b[01;34mpre-extracted\u001b[0m/   sample_submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls -L $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations_fr_test.csv   observations_us_test.csv\r\n",
      "observations_fr_train.csv  observations_us_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls $DATA_PATH/observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the observation ids of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for training: 1627475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>species_id</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observation_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10561949</th>\n",
       "      <td>45.705116</td>\n",
       "      <td>1.424622</td>\n",
       "      <td>241</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10131188</th>\n",
       "      <td>45.146973</td>\n",
       "      <td>6.416794</td>\n",
       "      <td>101</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799362</th>\n",
       "      <td>46.783695</td>\n",
       "      <td>-2.072855</td>\n",
       "      <td>700</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10392536</th>\n",
       "      <td>48.604866</td>\n",
       "      <td>-2.825003</td>\n",
       "      <td>1456</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10335049</th>\n",
       "      <td>48.815567</td>\n",
       "      <td>-0.161431</td>\n",
       "      <td>157</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 latitude  longitude  species_id subset\n",
       "observation_id                                         \n",
       "10561949        45.705116   1.424622         241  train\n",
       "10131188        45.146973   6.416794         101  train\n",
       "10799362        46.783695  -2.072855         700  train\n",
       "10392536        48.604866  -2.825003        1456  train\n",
       "10335049        48.815567  -0.161431         157  train"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training Dataset ###\n",
    "# let's load the data from file\n",
    "df_obs_fr = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "df_obs = pd.concat((df_obs_fr, df_obs_us))\n",
    "\n",
    "print(\"Number of observations for training: {}\".format(len(df_obs)))\n",
    "\n",
    "# let's have a look at the data\n",
    "df_obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the observation ids of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for testing: 36421\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observation_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10782781</th>\n",
       "      <td>43.601788</td>\n",
       "      <td>6.940195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10364138</th>\n",
       "      <td>46.241711</td>\n",
       "      <td>0.683586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10692017</th>\n",
       "      <td>45.181095</td>\n",
       "      <td>1.533459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10222322</th>\n",
       "      <td>46.938450</td>\n",
       "      <td>5.298678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10241950</th>\n",
       "      <td>45.017433</td>\n",
       "      <td>0.960736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 latitude  longitude\n",
       "observation_id                      \n",
       "10782781        43.601788   6.940195\n",
       "10364138        46.241711   0.683586\n",
       "10692017        45.181095   1.533459\n",
       "10222322        46.938450   5.298678\n",
       "10241950        45.017433   0.960736"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test Dataset ###\n",
    "df_obs_fr_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "df_obs_test = pd.concat((df_obs_fr_test, df_obs_us_test))\n",
    "\n",
    "print(\"Number of observations for testing: {}\".format(len(df_obs_test)))\n",
    "\n",
    "df_obs_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load suggested landcover alignment (only relevant if you're using landcover data later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   landcover_code  suggested_landcover_code suggested_landcover_label\n",
      "0               0                         0              Missing Data\n",
      "1               1                        11          Cultivated Crops\n",
      "2               2                        11          Cultivated Crops\n",
      "3               3                         6       Broad-leaved Forest\n",
      "4               4                         7         Coniferous Forest\n"
     ]
    }
   ],
   "source": [
    "df_suggested_landcover_alignment = pd.read_csv(DATA_PATH / \"metadata\" / \"landcover_suggested_alignment.csv\", sep=\";\")\n",
    "print(df_suggested_landcover_alignment.head())\n",
    "landcover_mapping = df_suggested_landcover_alignment[\"suggested_landcover_code\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Split Labels\n",
    "Retrieve the train/val split provided, and load the labels of the train and val set elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1587395 (97.5% of train observations)\n",
      "Validation set size: 40080 (2.5% of train observations)\n"
     ]
    }
   ],
   "source": [
    "obs_id_train = df_obs.index[df_obs[\"subset\"] == \"train\"].values\n",
    "obs_id_val = df_obs.index[df_obs[\"subset\"] == \"val\"].values\n",
    "\n",
    "y_train = df_obs.loc[obs_id_train][\"species_id\"].values\n",
    "y_val = df_obs.loc[obs_id_val][\"species_id\"].values\n",
    "\n",
    "n_val = len(obs_id_val)\n",
    "print(\"Training set size: {} ({:.1%} of train observations)\".format(len(y_train), len(y_train) / len(df_obs)))\n",
    "print(\"Validation set size: {} ({:.1%} of train observations)\".format(n_val, n_val / len(df_obs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Dataset ===\n",
      "There are 17031 unique labels.\n",
      "We have 93.21 observations per label on average.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Training Dataset ===\")\n",
    "print(\"There are {} unique labels.\".format(len(np.unique(y_train))))\n",
    "print(\"We have {:.2f} observations per label on average.\".format(len(obs_id_train)/len(np.unique(y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only retrieve the data belonging to a subset of all possible labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 1000\n",
    "obs_list = list()\n",
    "\n",
    "# iterate over a subset of the labels\n",
    "counter = 0\n",
    "for y in np.unique(y_train)[:subset_size]:\n",
    "    # for each label, retrieve all corresponding observation ids\n",
    "    obs = df_obs.index[df_obs[\"species_id\"] == y].values\n",
    "    obs_list.append(obs)\n",
    "    \n",
    "# we now have a numpy array of all observation ids corresponding to this subset of labels\n",
    "obs_id_train = np.concatenate(obs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write input pipeline to load batches as we train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Generator\n",
    "Since dataset is too large to load it all into memory once, we need to load it from disk in batches as we train. Such a generator can later be passed into model.fit() instead of a train and/or validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches_Generator(tf.keras.utils.Sequence) :\n",
    "  \n",
    "    def __init__(self, obs_ids, labels, batch_size) :\n",
    "        self.obs_ids = obs_ids\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # to make the generator thread safe \n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.obs_ids) / float(self.batch_size))).astype(int)\n",
    "  \n",
    "    # returns one batch\n",
    "    def __getitem__(self, idx) :\n",
    "        X_batch = list()\n",
    "        y_batch = list()\n",
    "\n",
    "        for i in range(idx * self.batch_size, (idx+1) * self.batch_size):\n",
    "            patch = load_patch(self.obs_ids[i], DATA_PATH, data='rgb')\n",
    "            X_batch.append(patch[0])\n",
    "            y_batch.append(self.labels[i])\n",
    "\n",
    "        with self.lock:\n",
    "            return np.asarray(X_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Simple Neural Network\n",
    "Let's create a first neural network as a baseline to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "# for distributed training (that is, using multiple GPUs for data parallelization)\n",
    "# https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a simple convolutional neural net\n",
    "def simple_model(input_shape, learning_rate=0.1):\n",
    "    \n",
    "    # for distributed training\n",
    "    with mirrored_strategy.scope():\n",
    "    \n",
    "        model = tf.keras.models.Sequential()\n",
    "\n",
    "        # 1. Preprocessing\n",
    "        # rescale inputs\n",
    "        model.add(tf.keras.layers.Rescaling(1./255))\n",
    "\n",
    "        # 2. Convolutional Layers\n",
    "        model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=input_shape, padding='valid'))\n",
    "        model.add(MaxPooling2D())\n",
    "\n",
    "        model.add(Conv2D(64, kernel_size=3, activation='relu', padding='valid'))\n",
    "        model.add(MaxPooling2D())\n",
    "\n",
    "        model.add(Conv2D(128, kernel_size=3, activation='relu', padding='valid'))\n",
    "        model.add(MaxPooling2D())\n",
    "\n",
    "        model.add(Conv2D(256, kernel_size=3, activation='relu', padding='valid'))\n",
    "\n",
    "        # from convolutional layers to dense layers\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "        # 3. Dense Layers\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "\n",
    "        # 4. Output Layer\n",
    "        model.add(Dense(17038, activation='softmax'))\n",
    "    \n",
    "    # compire the model\n",
    "    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "# create the network\n",
    "model = simple_model((256, 256, 3), learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings used when parallelizing the I/O Pipeline in model.fit().**\n",
    "\n",
    "I used the setting values in the cell below for a high-cpu vm on GCP with the following specs:\n",
    "- machine type: n1-highcpu-96 (96 CPU cores)\n",
    "- vCPUs to core ratio: 2 vCPUs per core (making a theoretical max value for num_threads of 96 * 2 = 192)\n",
    "- 4 x NVIDIA Tesla T4 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to tune the learning rate accordingly.\n",
    "BATCHSIZE = 512   \n",
    "\n",
    "# The maximun value for num_threads is dependent on amount of CPU cores:\n",
    "# amount of CPU cores * vCPUs to core ratio = theoretical max of NUM_THREADS\n",
    "NUM_THREADS = 140 \n",
    "\n",
    "# The more batches we prefetch, the less idle the GPUs will be. \n",
    "# To check GPU usage:\n",
    "# 1. Run nvidia-smi -l 1 from the terminal to monitor the GPU usage during training. \n",
    "# 2. Try to get close to 100% for all GPUs by adjusting the value below (and the two above). Due to the overhead\n",
    "#    from tf.distribute.MirroredStrategy(), you won't be able to consistently get 100% for all GPUs. But try to \n",
    "#    get close.\n",
    "# 3. Be aware that RAM limits the amount of batches you can prefetch.\n",
    "PRE_FETCH_NUM_BATCHES = int(NUM_THREADS * 2.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create generators that will read training / validation data from disk during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = Patches_Generator(obs_id_train, y_train, BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_generator = Patches_Generator(obs_id_val, y_val, BATCHSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an early stopping callback for when model converges\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.001, patience=5, \n",
    "                                              verbose=0, mode='auto', baseline=None, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "1224/3100 [==========>...................] - ETA: 15:30 - loss: 3781163.5000 - accuracy: 0.0033"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\n5 root error(s) found.\n  (0) UNKNOWN:  IndexError: index 1587395 is out of bounds for axis 0 with size 1587395\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 566, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n  File \"<ipython-input-13-79cb6022ac25>\", line 20, in __getitem__\n    patch = load_patch(self.obs_ids[i], DATA_PATH, data='rgb')\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\"\"\"\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 785, in get\n    raise e\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 776, in get\n    inputs = self.queue.get(block=True, timeout=5).get()\n\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 771, in get\n    raise self._value\n\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\n\n\t [[{{node PyFunc}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]\n\t [[div_no_nan_1/ReadVariableOp_3/_158]]\n  (1) UNKNOWN:  IndexError: index 1587395 is out of bounds for axis 0 with size 1587395\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 566, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n  File \"<ipython-input-13-79cb6022ac25>\", line 20, in __getitem__\n    patch = load_patch(self.obs_ids[i], DATA_PATH, data='rgb')\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\"\"\"\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 785, in get\n    raise e\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 776, in get\n    inputs = self.queue.get(block=True, timeout=5).get()\n\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 771, in get\n    raise self._value\n\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\n\n\t [[{{node PyFunc}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]\n\t [[cond/pivot_t/_4/_65]]\n  (2) UNKNOWN:  IndexError: index 1587395 is out of bounds for axis 0 with size 1587395\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 566, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n  File \"<ipython-input-13-79cb6022ac25>\", line 20, in __getitem__\n    patch = load_patch(self.obs_ids[i], DATA_PATH, data='rgb')\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\"\"\"\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 785, in get\n    raise e\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 776, in get\n    inputs = self.queue.get(block=True, timeout=5).get()\n\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 771, in get\n    raise self._value\n\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\n\n\t [[{{node PyFunc}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]\n\t [[Greater/_46]]\n  (3) UNKNOWN:  IndexError: index 1587395 is out of bounds for axis 0 with size 1587395\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 566, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n  File \"<ipython-input-13-79cb6022ac25>\", line 20, in __getitem__\n    patch = load_patch(self.obs_ids[i], DATA_PATH, data='rgb')\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\"\"\"\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 785, in get\n    raise e\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 776, in get\n    inputs = self.queue.get(block=True, timeout=5).get()\n\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 771, in get\n    raise self._value\n\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\n\n\t [[{{node PyFunc}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]\n\t [[Adam/Adam/group_deps/_241]]\n  (4) UNKNOWN:  IndexError: index 1587395 is out of bounds for axis 0 with size 1587395\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_uti [Op:__inference_train_function_5333]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mBATCHSIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# for parallelization of reading from disk (I/O) pipeline\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPRE_FETCH_NUM_BATCHES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_THREADS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\n5 root error(s) found.\n  (0) UNKNOWN:  IndexError: index 1587395 is out of bounds for axis 0 with size 1587395\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 566, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n  File \"<ipython-input-13-79cb6022ac25>\", line 20, in __getitem__\n    patch = load_patch(self.obs_ids[i], DATA_PATH, data='rgb')\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\"\"\"\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 785, in get\n    raise e\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 776, in get\n    inputs = self.queue.get(block=True, timeout=5).get()\n\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 771, in get\n    raise self._value\n\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\n\n\t [[{{node PyFunc}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]\n\t [[div_no_nan_1/ReadVariableOp_3/_158]]\n  (1) UNKNOWN:  IndexError: index 1587395 is out of bounds for axis 0 with size 1587395\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 566, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n  File \"<ipython-input-13-79cb6022ac25>\", line 20, in __getitem__\n    patch = load_patch(self.obs_ids[i], DATA_PATH, data='rgb')\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\"\"\"\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 785, in get\n    raise e\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 776, in get\n    inputs = self.queue.get(block=True, timeout=5).get()\n\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 771, in get\n    raise self._value\n\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\n\n\t [[{{node PyFunc}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]\n\t [[cond/pivot_t/_4/_65]]\n  (2) UNKNOWN:  IndexError: index 1587395 is out of bounds for axis 0 with size 1587395\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 566, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n  File \"<ipython-input-13-79cb6022ac25>\", line 20, in __getitem__\n    patch = load_patch(self.obs_ids[i], DATA_PATH, data='rgb')\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\"\"\"\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 785, in get\n    raise e\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 776, in get\n    inputs = self.queue.get(block=True, timeout=5).get()\n\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 771, in get\n    raise self._value\n\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\n\n\t [[{{node PyFunc}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]\n\t [[Greater/_46]]\n  (3) UNKNOWN:  IndexError: index 1587395 is out of bounds for axis 0 with size 1587395\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 566, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n  File \"<ipython-input-13-79cb6022ac25>\", line 20, in __getitem__\n    patch = load_patch(self.obs_ids[i], DATA_PATH, data='rgb')\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\"\"\"\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 785, in get\n    raise e\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 776, in get\n    inputs = self.queue.get(block=True, timeout=5).get()\n\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 771, in get\n    raise self._value\n\nIndexError: index 1587395 is out of bounds for axis 0 with size 1587395\n\n\n\t [[{{node PyFunc}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]\n\t [[Adam/Adam/group_deps/_241]]\n  (4) UNKNOWN:  IndexError: index 1587395 is out of bounds for axis 0 with size 1587395\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_uti [Op:__inference_train_function_5333]"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, epochs=100, steps_per_epoch=len(y_train)//BATCHSIZE, callbacks=[early_stop], \n",
    "                    # for parallelization of reading from disk (I/O) pipeline\n",
    "                    max_queue_size=PRE_FETCH_NUM_BATCHES, workers=NUM_THREADS, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_simple_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
